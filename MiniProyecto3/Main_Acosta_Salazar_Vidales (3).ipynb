{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "786b932b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "260c085811d82baa7adab1dfb951b187",
     "grade": false,
     "grade_id": "cell-422c5ba8508b3773",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Miniproyecto 3: Clasificación de animales\n",
    "\n",
    "## Entrega 4\n",
    "En las entregas anteriores, exploramos diversos tipos de descriptores que pueden ayudarnos a resolver la tarea de clasificación. Hasta ahora, hemos experimentado con el clasificador de vecino más cercano y las Máquinas de Soporte Vectorial (SVMs).\n",
    "\n",
    "En esta entrega, integraremos diferentes clasificadores dentro de un método de modelos de expertos. Los clasificadores que utilizaremos serán: SVMs, Random Forests, MLPs (Redes Neuronales Multicapa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d3e80e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar librerias\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from skimage.feature import hog\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from skimage.transform import resize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import correlate2d\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a167d5da",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c50b706247e646020633092ed524adf2",
     "grade": false,
     "grade_id": "cell-fb2c4b26d9dc45eb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "group = 3 # Cambiar por el número de grupo\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7932bc52",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb1759ab006571859526ee67cd4f12cb",
     "grade": false,
     "grade_id": "cell-f8975a6fbe3d6b4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: expecting '}' (1688003964.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 29\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(f\"Las classes que usarán en este Mini-proyecto serán:\\n{\", \".join(categories)}.\")\u001b[0m\n\u001b[1;37m                                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m f-string: expecting '}'\n"
     ]
    }
   ],
   "source": [
    "def determine_classes(group: int) -> list:\n",
    "    \"\"\"Función que determina las clases a utilizar en el proyecto.\n",
    "\n",
    "    Args:\n",
    "        group (int): Número de grupo.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista con los nombres de las clases a utilizar.\n",
    "    \"\"\"\n",
    "    random.seed(group)\n",
    "    categories = glob(os.path.join(\"dataset\", \"train\", \"*\"))\n",
    "    stats = [(os.path.basename(category), len(os.listdir(category))) for category in categories]\n",
    "    clusters = {}\n",
    "    for category, num_images in stats:\n",
    "        if num_images in clusters:\n",
    "            clusters[num_images].append(category)\n",
    "        else:\n",
    "            clusters[num_images] = [category]\n",
    "    categories = []\n",
    "    for cluster_categories in clusters.values():\n",
    "        categories += [random.choice(cluster_categories)]\n",
    "    return categories\n",
    "\n",
    "assert group is not None, \"Por favor, asigna un número de grupo a la variable 'group'.\"\n",
    "assert group > 0, \"El número de grupo debe ser mayor que 0.\"\n",
    "assert group <= 28, \"El número de grupo debe ser menor o igual que 28.\"\n",
    "\n",
    "categories = determine_classes(group)\n",
    "print(f\"Las classes que usarán en este Mini-proyecto serán:\\n{\", \".join(categories)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_classes(group: int) -> list:\n",
    "    \"\"\"Función que determina las clases a utilizar en el proyecto.\n",
    "\n",
    "    Args:\n",
    "        group (int): Número de grupo.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista con los nombres de las clases a utilizar.\n",
    "    \"\"\"\n",
    "    random.seed(group)\n",
    "    categories = glob(os.path.join(\"dataset\", \"train\", \"*\"))\n",
    "    stats = [(os.path.basename(category), len(os.listdir(category))) for category in categories]\n",
    "    clusters = {}\n",
    "    for category, num_images in stats:\n",
    "        if num_images in clusters:\n",
    "            clusters[num_images].append(category)\n",
    "        else:\n",
    "            clusters[num_images] = [category]\n",
    "    categories = []\n",
    "    for cluster_categories in clusters.values():\n",
    "        categories += [random.choice(cluster_categories)]\n",
    "    return categories\n",
    "\n",
    "assert group is not None, \"Por favor, asigna un número de grupo a la variable 'group'.\"\n",
    "assert group > 0, \"El número de grupo debe ser mayor que 0.\"\n",
    "assert group <= 28, \"El número de grupo debe ser menor o igual que 28.\"\n",
    "\n",
    "categories = determine_classes(group)\n",
    "print(f\"Las classes que usarán en este Mini-proyecto serán:\\n{', '.join(categories)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d172e843",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1e39677fed6c667983d9dde457e97ea",
     "grade": false,
     "grade_id": "cell-eb2d00722c2dc741",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class_names = sorted([\n",
    "    \"Sciurus carolinensis\",\n",
    "    \"Phoca vitulina\",\n",
    "    \"Otospermophilus variegatus\",\n",
    "    \"Tamias striatus\",\n",
    "    \"Canis latrans\"\n",
    "])\n",
    "\n",
    "dict_labels = {name: idx for idx, name in enumerate(class_names)}\n",
    "print(dict_labels) # Cambiar por un diccionario con las etiquetas de las clases\n",
    "\n",
    "# YOUR CODE HERE\n",
    "def get_data(subset: str) -> tuple[list[str], list[int]]:\n",
    "  \"\"\"Función que obtiene las rutas de las imágenes y sus etiquetas.\n",
    "\n",
    "  Args:\n",
    "      subset (str): Nombre del subconjunto de datos a utilizar.\n",
    "\n",
    "  Returns:\n",
    "      tuple[list, list]: Rutas de las imágenes y sus etiquetas.\n",
    "  \"\"\"\n",
    "  base_dir = os.path.join(\"dataset\", subset)\n",
    "  paths = [] # Cambiar por una lista con las rutas de las imágenes\n",
    "  labels = [] # Cambiar por una lista con las etiquetas de las imágenes\n",
    "\n",
    "  # YOUR CODE HERE\n",
    "  for class_name in class_names:\n",
    "        class_dir = os.path.join(base_dir, class_name)\n",
    "        if not os.path.exists(class_dir):\n",
    "            continue\n",
    "\n",
    "        # Encontrar todos los .jpg o .png\n",
    "        image_files = glob(os.path.join(class_dir, \"*.*\"))\n",
    "        for image_path in image_files:\n",
    "            paths.append(image_path)\n",
    "            labels.append(dict_labels[class_name])\n",
    "\n",
    "  return paths, labels\n",
    "\n",
    "train_paths, train_labels = get_data(\"train\")\n",
    "valid_paths, valid_labels = get_data(\"valid\")\n",
    "test_paths, test_labels = get_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69126618-33ee-404b-af0b-020e5ab74068",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0103f37bfe4217206ff01000f68793d8",
     "grade": true,
     "grade_id": "cell-651dfca4832b933a",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(train_paths) > 0, \"No se ha cargado ningún dato de entrenamiento\"\n",
    "assert len(train_paths) == 5163, \"El número de datos de entrenamiento no es correcto\"\n",
    "assert len(valid_paths) > 0, \"No se ha cargado ningún dato de validación\"\n",
    "assert len(valid_paths) == 1723, \"El número de datos de validación no es correcto\"\n",
    "assert len(test_paths) > 0, \"No se ha cargado ningún dato de test\"\n",
    "assert len(test_paths) == 1724, \"El número de datos de test no es correcto\"\n",
    "\n",
    "assert type(train_paths[0]) == str, \"Los datos de entrenamiento no tienen la forma correcta\"\n",
    "assert type(valid_paths[0]) == str, \"Los datos de validación no tienen la forma correcta\"\n",
    "assert type(test_paths[0]) == str, \"Los datos de test no tienen la forma correcta\"\n",
    "\n",
    "assert type(train_labels[0]) == int, \"Las etiquetas de entrenamiento no tienen la forma correcta\"\n",
    "assert type(valid_labels[0]) == int, \"Las etiquetas de validación no tienen la forma correcta\"\n",
    "assert type(test_labels[0]) == int, \"Las etiquetas de test no tienen la forma correcta\"\n",
    "\n",
    "assert len(dict_labels) == len(categories), \"El diccionario de labels no tiene la longitud correcta\"\n",
    "\n",
    "categories_sorted = sorted(categories)\n",
    "assert list(dict_labels.keys()) == categories_sorted, \"Las llaves del diccionario de labels no son correctas\"\n",
    "assert list(dict_labels.values()) == list(range(len(categories))), \"Los valores del diccionario de labels no son correctos\"\n",
    "\n",
    "assert min([dict_labels[train_paths[i].split(os.sep)[-2]] == train_labels[i] for i in range(len(train_paths))]), \"Las etiquetas de entrenamiento no coinciden\"\n",
    "assert min([dict_labels[valid_paths[i].split(os.sep)[-2]] == valid_labels[i] for i in range(len(valid_paths))]), \"Las etiquetas de validación no coinciden\"\n",
    "assert min([dict_labels[test_paths[i].split(os.sep)[-2]] == test_labels[i] for i in range(len(test_paths))]), \"Las etiquetas de test no coinciden\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1acaee",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8bb28d3d19724053d4bd5df34faa398f",
     "grade": false,
     "grade_id": "cell-10fa83a0be8f86bb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def processing(Im: np.ndarray, img_size: tuple[int] | None = None, color_space: str | None = None) -> np.ndarray:\n",
    "    \"\"\"Función que preprocesa una lista de imágenes.\n",
    "\n",
    "    Args:\n",
    "        Im_list (list[np.ndarray]): Lista con las imágenes a preprocesar.\n",
    "        img_size (tuple[int] | None): Tamaño de la imagen.\n",
    "        color_space (str | None): Espacio de color a utilizar. Puede ser \"rgb\", \"gray\", \"hsv\" o \"lab\".\n",
    "\n",
    "    Returns:\n",
    "        list[np.ndarray]: Lista con las imágenes preprocesadas.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    if img_size is not None:\n",
    "        Im = cv2.resize(Im, img_size)\n",
    "\n",
    "    # Convertir al espacio de color\n",
    "    if color_space is not None:\n",
    "        if color_space == \"rgb\":\n",
    "            if Im.ndim == 3:  # evadir convertir a escala de grises a RGB\n",
    "                Im = cv2.cvtColor(Im, cv2.COLOR_BGR2RGB)\n",
    "        elif color_space == \"gray\":\n",
    "            if Im.ndim == 3:  # solo convertir a escala de grises\n",
    "                Im = cv2.cvtColor(Im, cv2.COLOR_BGR2GRAY)\n",
    "        elif color_space == \"hsv\":\n",
    "            Im = cv2.cvtColor(Im, cv2.COLOR_BGR2HSV)\n",
    "        elif color_space == \"lab\":\n",
    "            Im = cv2.cvtColor(Im, cv2.COLOR_BGR2LAB)\n",
    "        else:\n",
    "            raise ValueError(f\"Espacio de Color no disponible: {color_space}\")\n",
    "\n",
    "    return Im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f241b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d96f9a1911cc95e6fdcae81a7901873",
     "grade": true,
     "grade_id": "cell-2ba399db8c73d160",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Im_list = [cv2.imread(path) for path in train_paths[:5]]\n",
    "img_size = (224, 224)\n",
    "color_space = \"rgb\"\n",
    "Im_proc = [processing(Im, img_size, color_space) for Im in Im_list]\n",
    "\n",
    "assert len(Im_proc) == len(Im_list), \"El número de imágenes preprocesadas no es correcto\"\n",
    "assert type(Im_proc) == list, \"Las imágenes preprocesadas no están en una lista\"\n",
    "assert type(Im_proc[0]) == np.ndarray, \"Las imágenes preprocesadas no son arreglos de numpy\"\n",
    "assert np.array(Im_proc).shape[1:] == (224, 224, 3), \"Las imágenes preprocesadas no tienen el tamaño correcto\"\n",
    "assert np.array_equal(cv2.cvtColor(cv2.resize(Im_list[0], (224, 224)), cv2.COLOR_BGR2RGB), Im_proc[0]), \"Las imágenes preprocesadas no son correctas\"\n",
    "\n",
    "color_space = \"gray\"\n",
    "Im_proc = [processing(Im, img_size, color_space) for Im in Im_list]\n",
    "assert np.array(Im_proc).shape[1:] == (224, 224), \"Las imágenes preprocesadas no tienen el tamaño correcto\"\n",
    "assert np.array_equal(cv2.cvtColor(cv2.resize(Im_list[0], (224, 224)), cv2.COLOR_BGR2GRAY), Im_proc[0]), \"Las imágenes preprocesadas no son correctas\"\n",
    "\n",
    "color_space = \"hsv\"\n",
    "Im_proc = [processing(Im, img_size, color_space) for Im in Im_list]\n",
    "assert np.array(Im_proc).shape[1:] == (224, 224, 3), \"Las imágenes preprocesadas no tienen el tamaño correcto\"\n",
    "assert np.array_equal(cv2.cvtColor(cv2.resize(Im_list[0], (224, 224)), cv2.COLOR_BGR2HSV), Im_proc[0]), \"Las imágenes preprocesadas no son correctas\"\n",
    "\n",
    "color_space = \"lab\"\n",
    "Im_proc = [processing(Im, img_size, color_space) for Im in Im_list]\n",
    "assert np.array(Im_proc).shape[1:] == (224, 224, 3), \"Las imágenes preprocesadas no tienen el tamaño correcto\"\n",
    "assert np.array_equal(cv2.cvtColor(cv2.resize(Im_list[0], (224, 224)), cv2.COLOR_BGR2LAB), Im_proc[0]), \"Las imágenes preprocesadas no son correctas\"\n",
    "\n",
    "Im_proc = [processing(Im, (256, 256)) for Im in Im_list]\n",
    "assert np.array(Im_proc).shape[1:] == (256, 256, 3), \"Las imágenes preprocesadas no tienen el tamaño correcto\"\n",
    "assert np.array_equal(cv2.resize(Im_list[0], (256, 256)), Im_proc[0]), \"Las imágenes preprocesadas no son correctas\"\n",
    "\n",
    "Im_proc = [processing(Im, None, \"lab\") for Im in Im_list]\n",
    "assert np.array_equal(cv2.cvtColor(Im_list[0], cv2.COLOR_BGR2LAB), Im_proc[0]), \"Las imágenes preprocesadas no son correctas\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f690ce8c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf94b9da13df92fba982c81f1e075783",
     "grade": false,
     "grade_id": "cell-8ecefcf337cc8827",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Parte 1.1: Modelo de expertos\n",
    "En entregas anteriores, nos hemos centrado en la experimentación con descriptores y en aumentar la dimensionalidad de estos agregando información adicional, con la expectativa de mejorar los resultados. Otra estrategia interesante es entrenar diferentes modelos especializados en descriptores específicos. Por ejemplo, podemos entrenar un modelo experto en diferenciar imágenes por su forma y otro por su color. Cada modelo hará su predicción, y al final realizaremos una votación entre ellos para obtener la clasificación final.\n",
    "\n",
    "Para este experimento, utilizaremos descriptores sencillos como forma o pirámide de colores. Antes de iniciar, copiaremos algunas funciones necesarias para realizar las pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbbe6ab",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16fffc5e758db3efe6d873721fff1e9e",
     "grade": false,
     "grade_id": "cell-3b37701229021987",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "#USAR CUPY/CODA EN ESTA FUNCION\n",
    "\n",
    "def color_hist(Im: np.ndarray, type_h: str, bins: int) -> np.ndarray:\n",
    "    \"\"\"Optimized version using CuPy for GPU histogram computation.\"\"\"\n",
    "    Im_gpu = cp.asarray(Im)  # enviar a GPU\n",
    "\n",
    "    if type_h == \"concat\":\n",
    "        hists = []\n",
    "        for c in range(Im_gpu.shape[2]):\n",
    "            ch_data = Im_gpu[:, :, c].ravel()\n",
    "            hist = cp.histogram(ch_data, bins=bins, range=(0, 256))[0]\n",
    "            hists.append(hist)\n",
    "        h = cp.concatenate(hists)\n",
    "    elif type_h == \"joint\":\n",
    "        intensities = Im_gpu.reshape(-1, 3)\n",
    "        h = cp.histogramdd(intensities, bins=(bins, bins, bins),\n",
    "                           range=((0, 256), (0, 256), (0, 256)))[0].ravel()\n",
    "    else:\n",
    "        raise ValueError(\"type_h must be 'concat' or 'joint'\")\n",
    "\n",
    "    h = h / h.sum()  # Normalizar\n",
    "    return cp.asnumpy(h)  # retornar\n",
    "\n",
    "\n",
    "def Color_pyramid(Im: np.ndarray, patch_sizes: list[tuple[int]], type_h: str, bins: int) -> np.ndarray:\n",
    "    final_histogram = []\n",
    "    for patch_h, patch_w in patch_sizes:\n",
    "        num_rows = Im.shape[0] // patch_h\n",
    "        num_cols = Im.shape[1] // patch_w\n",
    "        for i in range(num_rows):\n",
    "            for j in range(num_cols):\n",
    "                patch = Im[i * patch_h:(i + 1) * patch_h, j * patch_w:(j + 1) * patch_w]\n",
    "                patch_hist = color_hist(patch, type_h, bins)\n",
    "                final_histogram.append(patch_hist)\n",
    "    return np.concatenate(final_histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6531fe20-ed56-45fc-bfc4-2829396d4e82",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ab2cf0abe654dfe0545dc4ae7a3b929",
     "grade": true,
     "grade_id": "cell-dcf60a9caeb70cbc",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "img=processing(cv2.imread(os.path.join(\".\", \"dataset\", \"train\", \"Canis latrans\", \"f494fab00ebb2d87c8d3bf5215560d54.jpg\")),  (256, 256), 'rgb')\n",
    "h_pyramid_cat=Color_pyramid(img,[(256, 256), (128, 128), (64, 64)],'concat',10)\n",
    "h_pyramid_joint=Color_pyramid(img,[(256, 256), (128, 128), (64, 64)],'joint',5)\n",
    "\n",
    "fig,ax=plt.subplots(2,2,figsize=(10,10))\n",
    "ax[0,0].imshow(img)\n",
    "ax[0,0].axis('off')\n",
    "ax[0,0].set_title('Imagen')\n",
    "ax[0,1].hist(list(range(len(h_pyramid_cat))), list(range(len(h_pyramid_cat - 1))), weights = h_pyramid_cat)\n",
    "ax[0,1].set_title('Histograma de piramide')\n",
    "\n",
    "assert len(h_pyramid_cat)==30*(1+4+16), 'La longitud del descriptor final debe ser igual a la longitud del histograma concatenado multiplicado por la cantidad de cuadrantes dentro de la imagen'\n",
    "assert np.isclose(np.sum(h_pyramid_cat),(1+4+16),0.01), 'En total se tienen 64 histogramas normalizados juntos, la suma de todo debe ser 64'\n",
    "assert len(h_pyramid_joint)==5**3*(1+4+16), 'La longitud del descriptor final debe ser igual a la longitud del histograma conjunto multiplicado por la cantidad de cuadrantes dentro de la imagen'\n",
    "assert np.isclose(np.sum(h_pyramid_joint),(1+4+16),0.01), 'En total se tienen 64 histogramas normalizados juntos, la suma de todo debe ser 64'\n",
    "a=color_hist(img[64:64+64,64:64+64,:],'concat',10)\n",
    "K=10\n",
    "b=h_pyramid_cat[K*30:K*30+30]\n",
    "\n",
    "ax[1,0].imshow(img[64:64+64,64:64+64,:])\n",
    "ax[1,0].axis('off')\n",
    "ax[1,0].set_title('Recorte')\n",
    "ax[1,1].hist(list(range(len(h_pyramid_cat[K*30:K*30+30]))), list(range(len(h_pyramid_cat[K*30:K*30+30] - 1))), weights = h_pyramid_cat[K*30:K*30+30])\n",
    "ax[1,1].set_title('Histograma de recorte')\n",
    "\n",
    "assert np.array_equal(a,b), 'El histograma concatenado de el cuadrante 17 de la imagen debe ser igual la parte 17 del histograma en piramide'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ee09b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8326eb6c214a492e26def72318c3bcf1",
     "grade": false,
     "grade_id": "cell-559149dacbc12d1e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### Parte 1.2.1: SVM\n",
    "Obtén el descriptor de forma de todas las imágenes del subconjunto de entrenamiento y realiza el mismo proceso para obtener el descriptor de color mediante un histograma concatenado en pirámide. Luego, extrae los descriptores de todas las imágenes de validación para evaluar el modelo. No olvides incluir la lista de etiquetas (labels) correspondientes para cada conjunto de datos, tanto para entrenamiento como para validación.\n",
    "\n",
    "**Nota:** Recuerde que para ser consistentes en los descriptores debe utilizar la función `resize` para cambiar el tamaño de las imágenes a $256\\times256$. Puede utilizar otro tamaño si considera necesario pero debe ser consistente a lo largo de toda la entrega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49eb9d7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "209ad9875527e8e1a7055c254d3be339",
     "grade": false,
     "grade_id": "cell-db299d77039d7588",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from skimage.feature import hog\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "import numpy as np\n",
    "# === Parámetros ===\n",
    "resize_shape = (256, 256)\n",
    "patch_sizes = [(256, 256), (128, 128), (64, 64)]\n",
    "bins = 8\n",
    "hog_params = dict(\n",
    "    orientations=9,\n",
    "    pixels_per_cell=(8, 8),\n",
    "    cells_per_block=(2, 2),\n",
    "    visualize=False,\n",
    "    channel_axis=-1\n",
    ")\n",
    "\n",
    "# === Inicializar listas ===\n",
    "shape_des_train = []\n",
    "color_des_train = []\n",
    "shape_des_valid = []\n",
    "color_des_valid = []\n",
    "train_images_num = len(train_paths)\n",
    "# === EXTRACCIÓN DE DESCRIPTORES: ENTRENAMIENTO ===\n",
    "print(\"Extrayendo descriptores de entrenamiento...\")\n",
    "for img_path in tqdm(train_paths, desc=\"Train\"):\n",
    "    img = cv2.imread(img_path)\n",
    "    img_rgb = processing(img, resize_shape, \"rgb\")\n",
    "\n",
    "    # CuPy con Color Pyramid descriptor\n",
    "    color_des_train.append(Color_pyramid(img_rgb, patch_sizes, \"concat\", bins))\n",
    "\n",
    "    # HOG descriptor \n",
    "    resized_hog = resize(img_rgb, (128, 64))\n",
    "    shape_des_train.append(hog(resized_hog, **hog_params))\n",
    "\n",
    "# === EXTRACCIÓN DE DESCRIPTORES: VALIDACIÓN ===\n",
    "print(\"Extrayendo descriptores de validación...\")\n",
    "for img_path in tqdm(valid_paths, desc=\"Validation\"):\n",
    "    img = cv2.imread(img_path)\n",
    "    img_rgb = processing(img, resize_shape, \"rgb\")\n",
    "\n",
    "    color_des_valid.append(Color_pyramid(img_rgb, patch_sizes, \"concat\", bins))\n",
    "\n",
    "    resized_hog = resize(img_rgb, (128, 64))\n",
    "    shape_des_valid.append(hog(resized_hog, **hog_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5a0d1-f390-403a-9331-18111049b029",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6a683dd228a650b6e6601797f753a7a",
     "grade": true,
     "grade_id": "cell-357f723dca9ea705",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(train_labels) == train_images_num, 'La cantidad de etiquetas que tiene debe ser igual a la cantidad de imagenes que usted especificó'\n",
    "assert len(shape_des_train) == len(color_des_train) and len(shape_des_valid) == len(color_des_valid), 'Usted debe tener la misma cantidad de descriptores en todas las listas'\n",
    "assert len(shape_des_train[0]) > 1, 'El descriptor debe tener más de una dimensión'\n",
    "assert len(color_des_train[0]) > 1, 'El descriptor debe tener más de una dimensión'\n",
    "assert len(color_des_train[0]) <= 7056, 'Su descriptor unimodal de color tiene demasiadas dimensiones'\n",
    "assert len(shape_des_train[0]) <= 7056, 'Su descriptor unimodal de forma tiene demasiadas dimensiones'\n",
    "assert len(np.unique(train_labels)) == 5, 'Solo deben haber 5 etiquetas'\n",
    "assert len(np.unique(valid_labels)) == 5, 'Solo deben haber 5 etiquetas'\n",
    "\n",
    "assert len(valid_labels) == 1723, 'Usted debe sacar el descriptor de todas las imagenes de validación'\n",
    "assert len(color_des_train[0]) == len(color_des_valid[0]), 'El descriptor en train y valid debe ser igual'\n",
    "assert len(shape_des_train[0]) == len(shape_des_valid[0]), 'El descriptor en train y valid debe ser igual'\n",
    "\n",
    "print(f\"Las dimensiones de su descriptor de color son {np.array(color_des_train).shape[1]}\")\n",
    "print(f\"Las dimensiones de su descriptor de forma son {np.array(shape_des_train).shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c049c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a82f16011bf3761196f17025c4e98028",
     "grade": false,
     "grade_id": "cell-39c301d86ef643af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Luego, entrene un modelo SVM **con el parametro de probability=True**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67452e60",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50c36a0138817504c531823134aaa4f0",
     "grade": false,
     "grade_id": "cell-9ddda99bb230a3c1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "svm_color='' #modelo SVM entrenado con color\n",
    "svm_shape='' #modelo SVM entrenado con forma\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#  Modelo SVM entrenado con color\n",
    "svm_color = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVC(probability=True, random_state=42)\n",
    ")\n",
    "svm_color.fit(color_des_train, train_labels)\n",
    "\n",
    "# Modelo SVM entrenado con forma\n",
    "svm_shape = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVC(probability=True, random_state=42)\n",
    ")\n",
    "svm_shape.fit(shape_des_train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f5619",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb2c5a0ae6a31dbed54330a4f9d122a8",
     "grade": false,
     "grade_id": "cell-b593fc49b218ca84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ya tenemos dos modelos, cada uno especializado en un tipo de descriptor. Ahora utilizaremos el método `.predict_proba` para obtener la probabilidad de cada clase según la predicción de cada modelo. A continuación, promediaremos las probabilidades de ambos modelos para cada clase de forma independiente. Por ejemplo, si una predicción devuelve las probabilidades [0.1, 0.4, 0.5] y la otra [0.3, 0.3, 0.4], el promedio final será [0.2, 0.35, 0.45]. Finalmente, selecciona la etiqueta con la mayor probabilidad.\n",
    "\n",
    "Primero, realiza las predicciones utilizando el método `.predict_proba` para cada uno de los modelos expertos y guárdalas en las variables correspondientes en la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f470d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2055abea07aca56ae081bf4fee39b0b8",
     "grade": false,
     "grade_id": "cell-8b10f602bc1f8dfb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "proba_color_valid=[] #lista de probabilidades de todas las imagenes de validación en color \n",
    "proba_shape_valid=[] #lista de probabilidades de todas las imagenes de validación en forma\n",
    "\n",
    "# YOUR CODE HERE\n",
    "proba_color_valid=svm_color.predict_proba(color_des_valid) #lista de probabilidades de todas las imagenes de validación en color\n",
    "proba_shape_valid=svm_shape.predict_proba(shape_des_valid) #lista de probabilidades de todas las imagenes de validación en forma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40138131",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "507ab943971fff19723e05ab2d42818a",
     "grade": true,
     "grade_id": "cell-4b1cdeb4faca8d4b",
     "locked": true,
     "points": 0.7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(proba_color_valid)==1723, 'Debería tener 1723 datos de predicción'\n",
    "assert len(proba_shape_valid)==1723, 'Debería tener 1723 datos de predicción'\n",
    "assert np.isclose(np.mean(np.sum(proba_color_valid,axis=1)),1), 'La suma de las probabilidades debe ser 1'\n",
    "assert svm_color.n_features_in_==len(color_des_train[0]), 'Usted entrenó el modelo de color con un descriptor diferente al de color'\n",
    "assert svm_shape.n_features_in_==len(shape_des_train[0]), 'Usted entrenó el modelo de forma con un descriptor diferente al de forma'\n",
    "assert not np.array_equal(proba_color_valid,proba_shape_valid), 'Sus probabilidades no son diferentes'\n",
    "assert np.array_equal(proba_color_valid[0],svm_color.predict_proba([color_des_valid[0]])[0]), 'Las probabilidades de color estan mal calculadas'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e03e27e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c42861bedd8471dd99f88d1f4397030",
     "grade": false,
     "grade_id": "cell-b91d7e37b0c40c41",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora, promedia las probabilidades obtenidas para cada una de las clases. Finalmente, para cada imagen, la predicción final será la clase con la mayor probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec2575",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3a4eb350851942b8c72b6a413e3e16d",
     "grade": false,
     "grade_id": "cell-a62f89b060fa264c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "final_predict=[] #predicción final por expertos\n",
    "color_predict=[] #predicción final por color\n",
    "shape_predict=[] #predicción final por forma\n",
    "# YOUR CODE HERE\n",
    "\n",
    "color_predict = np.argmax(proba_color_valid, axis=1)\n",
    "shape_predict = np.argmax(proba_shape_valid, axis=1)\n",
    "\n",
    "# Promedio de las probabilidades\n",
    "average_proba = (proba_color_valid + proba_shape_valid) / 2\n",
    "\n",
    "# Predicción final basada en el promedio de probabilidades\n",
    "final_predict = np.argmax(average_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95056f14",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2014996d37d51f86bf2be86e56144508",
     "grade": true,
     "grade_id": "cell-2b9a54fe366dd584",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "color_p=proba_color_valid[0]\n",
    "shape_p=proba_shape_valid[0]\n",
    "\n",
    "assert np.isclose(((color_p+shape_p)/2)[final_predict[0]],np.max(((color_p+shape_p)/2))), 'La probabilidad maxima no fue seleccionada como la etiqueta de predicción final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f4778f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d29608c7ee4e619a4b87ca706077ca3",
     "grade": false,
     "grade_id": "cell-3e7a17923889c4af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "f_color=f1_score(valid_labels,color_predict,average='macro')\n",
    "f_shape=f1_score(valid_labels,shape_predict,average='macro')\n",
    "f_final=f1_score(valid_labels,final_predict,average='macro')\n",
    "\n",
    "print(f'color: {f_color}')\n",
    "print(f'forma: {f_shape}')\n",
    "print(f'final: {f_final}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad878e91",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d03d22ebc3c31fffa09ab7bf27790e4d",
     "grade": true,
     "grade_id": "cell-5accacc277b07c59",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert f_color<0.8, 'Usted esta validando con los mismos datos que entrenó'\n",
    "assert f_shape<0.8, 'Usted esta validando con los mismos datos que entrenó'\n",
    "assert f_final<0.8, 'Usted esta validando con los mismos datos que entrenó'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2a7606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_color= precision_score(valid_labels, color_predict, average='macro')\n",
    "recall_color = recall_score(valid_labels, color_predict, average='macro')\n",
    "\n",
    "print('Color')\n",
    "print(f'Precisión: {precision_color}')\n",
    "print(f'Cobertura: {recall_color}')\n",
    "\n",
    "\n",
    "precision_shape= precision_score(valid_labels,shape_predict, average='macro')\n",
    "recall_shape = recall_score(valid_labels,shape_predict, average='macro')\n",
    "\n",
    "print('Forma')\n",
    "print(f'Precisión: {precision_shape}')\n",
    "print(f'Cobertura: {recall_shape}')\n",
    "\n",
    "\n",
    "precision_final = precision_score(valid_labels,final_predict, average='macro')\n",
    "recall_final = recall_score(valid_labels,final_predict, average='macro')\n",
    "\n",
    "print('Final')\n",
    "print(f'Precisión: {precision_final}')\n",
    "print(f'Cobertura: {recall_final}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df7638",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39c363a7d6c23d4ac534fb1128e19205",
     "grade": false,
     "grade_id": "cell-9878dd3521b6ebc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### Parte 1.2.2: Exploración con otros clasificadores\n",
    "En esta parte, repetiremos el mismo procedimiento que en la sección anterior, pero cambiando el clasificador de SVM a Random Forest o Redes Neuronales (MLP). Los descriptores ya están calculados, por lo que ahora solo necesitas entrenar un nuevo modelo de Random Forest y MLP utilizando sklearn, y luego calcular las probabilidades de predicción para cada clase.\n",
    "\n",
    "**Nota:** El MLP se entrena en un número determinado de iteraciones. El número predeterminado de iteraciones es 200, pero puede que el modelo no converja en ese tiempo. Aumenta el número de iteraciones si es necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc1915",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fb687a1fb3ade997642323f15afc610",
     "grade": false,
     "grade_id": "cell-183254655b60e007",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "RF_color='' #modelo RF entrenado con color\n",
    "RF_shape='' #modelo RF entrenado con forma\n",
    "\n",
    "MLP_color='' #modelo MLP entrenado con color\n",
    "MLP_shape='' #modelo MLP entrenado con forma\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Modelos de Random Forest\n",
    "RF_color = RandomForestClassifier(n_estimators=100, random_state=30)\n",
    "RF_color.fit(color_des_train, train_labels)\n",
    "\n",
    "RF_shape = RandomForestClassifier(n_estimators=100, random_state=30)\n",
    "RF_shape.fit(shape_des_train, train_labels)\n",
    "\n",
    "# Modelos de MLP\n",
    "MLP_color = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=30)#modelo MLP entrenado con color\n",
    "MLP_color.fit(color_des_train, train_labels)\n",
    "\n",
    "MLP_shape = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=30)#modelo MLP entrenado con forma\n",
    "MLP_shape.fit(shape_des_train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5049cee5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fb38146847363044a1aa076d3a6dabc",
     "grade": false,
     "grade_id": "cell-4f4132e02431d807",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora realizaremos la predicción de probabilidades para cada modelo y para cada imagen. Nuevamente, utilizaremos el método `.predict_proba` para obtener las probabilidades de predicción de cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c877f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7cacf5d1d7a920e478c139d06a122cc7",
     "grade": false,
     "grade_id": "cell-ba85d96f07f0bd0e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "proba_color_valid_RF='' #lista de probabilidades de color en validación Random Forest\n",
    "proba_shape_valid_RF='' #lista de probabilidades de forma en validación Random Forest\n",
    "\n",
    "proba_color_valid_MLP='' #lista de probabilidades de color en validación MLP\n",
    "proba_shape_valid_MLP='' #lista de probabilidades de forma en validación MLP\n",
    "\n",
    "# YOUR CODE HERE\n",
    "proba_color_valid_RF = RF_color.predict_proba(color_des_valid)\n",
    "proba_shape_valid_RF = RF_shape.predict_proba(shape_des_valid)\n",
    "\n",
    "proba_color_valid_MLP = MLP_color.predict_proba(color_des_valid)\n",
    "proba_shape_valid_MLP = MLP_shape.predict_proba(shape_des_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0cf58b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c1a5fc9b04be0382ed1a0300d3a1343",
     "grade": true,
     "grade_id": "cell-97afcc0ad8f08b9d",
     "locked": true,
     "points": 0.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(proba_color_valid_RF)==1723, 'Debería tener 500 datos de predicción'\n",
    "assert len(proba_shape_valid_RF)==1723, 'Debería tener 500 datos de predicción'\n",
    "assert len(proba_color_valid_MLP)==1723, 'Debería tener 500 datos de predicción'\n",
    "assert len(proba_shape_valid_MLP)==1723, 'Debería tener 500 datos de predicción'\n",
    "assert np.isclose(np.mean(np.sum(proba_color_valid_RF,axis=1)),1), 'La suma de las probabilidades debe ser 1'\n",
    "assert np.isclose(np.mean(np.sum(proba_color_valid_MLP,axis=1)),1), 'La suma de las probabilidades debe ser 1'\n",
    "assert RF_color.n_features_in_==len(color_des_train[0]), 'Usted entrenó el modelo de color con un descriptor diferente al de color'\n",
    "assert MLP_shape.n_features_in_==len(shape_des_train[0]), 'Usted entrenó el modelo de forma con un descriptor diferente al de forma'\n",
    "assert not np.array_equal(proba_color_valid_RF,proba_shape_valid_RF), 'Sus probabilidades no son diferentes'\n",
    "assert not np.array_equal(proba_color_valid_MLP,proba_shape_valid_MLP), 'Sus probabilidades no son diferentes'\n",
    "assert np.array_equal(proba_color_valid_RF[0],RF_color.predict_proba([color_des_valid[0]])[0]), 'Las probabilidades de color estan mal calculadas'\n",
    "assert np.mean(proba_color_valid_MLP[0]-MLP_color.predict_proba([color_des_valid[0]])[0])<0.01, 'Las probabilidades de color estan mal calculadas'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088326cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fddca5281c27bab65a0ed9766c11859c",
     "grade": false,
     "grade_id": "cell-6e26e6e3561b7acb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finalmente, haremos las predicciones normales utilizando el método `.predict`. Luego, compararemos estas predicciones con las predicciones obtenidas a partir de las probabilidades promediadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191d4a58",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afc74433a58f63e8027eaad8480fa673",
     "grade": false,
     "grade_id": "cell-009c3aaf17547a9b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "color_predict_RF='' #prediccion del modelo random forest de color\n",
    "color_predict_MLP='' #prediccion del modelo MLP de color\n",
    "shape_predict_RF='' #prediccion del modelo random forest de forma\n",
    "shape_predict_MLP='' #prediccion del modelo MLP de forma\n",
    "\n",
    "experto_MLP_predict='' #modelo experto de MLP unicamente\n",
    "experto_RF_predict='' #modelo experto de RF unicamente\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Paramétros\n",
    "rf_models = [RF_color, RF_shape]\n",
    "mlp_models = [MLP_color, MLP_shape]\n",
    "rf_inputs = [color_des_valid, shape_des_valid]\n",
    "mlp_inputs = [color_des_valid, shape_des_valid]\n",
    "\n",
    "# Predicción de probabilidades para RF y MLP\n",
    "proba_color_valid_RF, proba_shape_valid_RF = [model.predict_proba(data) for model, data in zip(rf_models, rf_inputs)]\n",
    "proba_color_valid_MLP, proba_shape_valid_MLP = [model.predict_proba(data) for model, data in zip(mlp_models, mlp_inputs)]\n",
    "\n",
    "# Predicciones\n",
    "color_predict_RF, shape_predict_RF = [model.predict(data) for model, data in zip(rf_models, rf_inputs)]\n",
    "color_predict_MLP, shape_predict_MLP = [model.predict(data) for model, data in zip(mlp_models, mlp_inputs)]\n",
    "\n",
    "# Modelos expertos\n",
    "avg_proba_RF = (proba_color_valid_RF + proba_shape_valid_RF) / 2\n",
    "avg_proba_MLP = (proba_color_valid_MLP + proba_shape_valid_MLP) / 2\n",
    "\n",
    "experto_RF_predict = np.argmax(avg_proba_RF, axis=1)\n",
    "experto_MLP_predict = np.argmax(avg_proba_MLP, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79b788c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27f5d4325f0dd500c5ffa4b141918ade",
     "grade": true,
     "grade_id": "cell-4eabdcce5392a58b",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "color_p=proba_color_valid_RF[0]\n",
    "shape_p=proba_shape_valid_RF[0]\n",
    "\n",
    "assert np.isclose(((color_p+shape_p)/2)[experto_RF_predict[0]],np.max(((color_p+shape_p)/2))), 'La probabilidad maxima no fue seleccionada como la etiqueta de predicción final'\n",
    "\n",
    "color_p=proba_color_valid_MLP[0]\n",
    "shape_p=proba_shape_valid_MLP[0]\n",
    "\n",
    "assert np.isclose(((color_p+shape_p)/2)[experto_MLP_predict[0]],np.max(((color_p+shape_p)/2))), 'La probabilidad maxima no fue seleccionada como la etiqueta de predicción final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216bf637",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33c33cd6023145af399f77d212f0a2d5",
     "grade": false,
     "grade_id": "cell-f38af34a8366db70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "f_color_RF=f1_score(valid_labels,color_predict_RF,average='macro')\n",
    "f_shape_RF=f1_score(valid_labels,shape_predict_RF,average='macro')\n",
    "f_experto_RF=f1_score(valid_labels,experto_RF_predict,average='macro')\n",
    "\n",
    "f_color_MLP=f1_score(valid_labels,color_predict_MLP,average='macro')\n",
    "f_shape_MLP=f1_score(valid_labels,shape_predict_MLP,average='macro')\n",
    "f_experto_MLP=f1_score(valid_labels,experto_MLP_predict,average='macro')\n",
    "\n",
    "print('Random Forest')\n",
    "print(f'color: {f_color_RF}')\n",
    "print(f'forma: {f_shape_RF}')\n",
    "print(f'final: {f_experto_RF}')\n",
    "print('MLP')\n",
    "print(f'color: {f_color_MLP}')\n",
    "print(f'forma: {f_shape_MLP}')\n",
    "print(f'final: {f_experto_MLP}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e27ad66",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "935710d1da36321598b1d39d00bd5fce",
     "grade": false,
     "grade_id": "cell-2d91666c0f02efaf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Por último, selecciona el mejor modelo para cada descriptor (entre Random Forest y MLP) y utilízalo como parte del modelo de expertos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b61cb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bc9e4d976f139243446103430e45aca",
     "grade": false,
     "grade_id": "cell-b82303ff3b735f08",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "f_experto_final= f_experto_RF #f medida de el modelo de expertos final\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c04362-0234-493a-b5e8-8c2575a36600",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4271f1491dd6524301f36c4a86cfcec5",
     "grade": true,
     "grade_id": "cell-483a7f6e678880ec",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert f_experto_final > 0.55, 'Su f medida final no es la esperada'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9566579e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "796508671dc011213415304b40efac59",
     "grade": false,
     "grade_id": "cell-ef399b662476d54e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Parte 2: Modelo final\n",
    "Hemos aprendido todas las herramientas necesarias para crear un modelo de clasificación de imágenes utilizando descriptores y clasificadores. Es importante destacar que, en el área de computer vision, el objetivo no es simplemente probar muchos descriptores hasta obtener el mejor modelo, sino pensar teóricamente cuál es la mejor forma de representar una imagen.\n",
    "\n",
    "En entregas anteriores, hemos explorado diferentes metodologías para implementar un modelo multi-descriptor en la clasificación de imágenes. Ahora, es el turno de tu grupo de diseñar el modelo final.\n",
    "\n",
    "**Instrucciones:**\n",
    "1. Utiliza el modelo que escogiste como baseline y experimenta con él agregando información de diferentes descriptores multimodales (color, forma, textura). No es necesario usar los tres descriptores a la vez; puedes probar combinaciones de dos.\n",
    "\n",
    "2. Una vez obtengas un modelo que supere el baseline, prueba tu nuevo modelo utilizando los clasificadores SVM, Random Forest, y MLP, realizando una experimentación exhaustiva con los hiperparámetros.\n",
    "\n",
    "3. Finalmente, selecciona el mejor modelo posible basado en los resultados de la validación.\n",
    "\n",
    "4. Crea una función Demo que reciba una imagen como parámetro, extraiga el o los descriptores de la imagen y clasifique esta imagen utilizando el clasificador final entrenado. El código debe generar automáticamente las predicciones del dataset de Test usando la función Demo.\n",
    "\n",
    "**Nota:** Utiliza pickle para guardar el clasificador final entrenado y adjúntalo en la entrega. Si utilizaste textones, incluye también el modelo entrenado de KMeans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6813ad63",
   "metadata": {},
   "source": [
    "#### Experimentacion:\n",
    "\n",
    "Cargamos el baseline obtenido en la entrega pasada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el baseline obtenido antes\n",
    "\n",
    "train_multimodal_desc = \"\" #Descriptor multimodal de entrenamiento\n",
    "val_multimodal_desc = \"\" #Descriptor multimodal de validación\n",
    "svm_multimodal_desc = \"\" #Modelo SVM entrenado con descriptor multimodal\n",
    "predict_svm_multimodal_desc = \"\" #Predicción de modelo SVM entrenado con descriptor multimodal\n",
    "f_multimodal_desc = \"\" #F medida de modelo SVM entrenado con descriptor multimodal\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "#Concatenar cada descriptor de imágen\n",
    "train_multimodal_desc = [np.concatenate([color, hog]) for color, hog in zip(color_des_train, shape_des_train)]\n",
    "val_multimodal_desc = [np.concatenate([color, hog]) for color, hog in zip(color_des_valid, shape_des_valid)]\n",
    "\n",
    "# Entrenar el modelo SVM en un descriptor multimodal\n",
    "svm_multimodal_desc = SVC(kernel='rbf', C=10.0)\n",
    "svm_multimodal_desc.fit(train_multimodal_desc, train_labels)\n",
    "\n",
    "# Predecir en el set de validación\n",
    "predict_svm_multimodal_desc = svm_multimodal_desc.predict(val_multimodal_desc)\n",
    "\n",
    "# Calcular la F medida\n",
    "_, _, f_multimodal_desc, _ = precision_recall_fscore_support(\n",
    "    valid_labels, predict_svm_multimodal_desc, average='macro'\n",
    ")\n",
    "print(f\"El valor de la F medida de su modelo multimodal es {f_multimodal_desc}\")\n",
    "\n",
    "baseline = f_multimodal_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6790902",
   "metadata": {},
   "source": [
    "**Ahora, extraeremos los textones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e43d57",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb5a4a9c95f1dc6b23a3a89d975913ee",
     "grade": false,
     "grade_id": "cell-ed18db3ffff5c4e6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Experimentación\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#Descriptor de textura\n",
    "\n",
    "filterbank='' #arreglo de filtros\n",
    "\n",
    "#Se carga la funcion filterbank.mat\n",
    "mat_data = loadmat(\"filterbank.mat\")\n",
    "\n",
    "#Añadir la llave \"filterbank\"\n",
    "filterbank = mat_data[\"filterbank\"]\n",
    "\n",
    "from scipy.signal import correlate2d\n",
    "\n",
    "def filter_map(Im: np.ndarray, filterbank: np.ndarray) -> torch.tensor:\n",
    "    \"\"\"Funcion que aplica un banco de filtros a una imagen\n",
    "\n",
    "    Args:\n",
    "        Im (numpy.ndarray): Imagen de entrada RGB\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Resultado de aplicar el banco de filtros a la imagen\n",
    "    \"\"\"\n",
    "    if len(Im.shape) == 3:\n",
    "        Im = cv2.cvtColor(Im, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dtype = torch.float32\n",
    "\n",
    "    Im = torch.tensor(Im, dtype=dtype, device=device)\n",
    "    Im = Im[None, None]\n",
    "    filterbank = torch.tensor(filterbank, dtype=dtype, device=device)\n",
    "    filterbank = filterbank.permute(2, 0, 1)[:, None]\n",
    "    convolution = torch.nn.functional.conv2d(Im, filterbank, padding=\"same\")\n",
    "    convolution = convolution.permute(0, 2, 3, 1)\n",
    "    new_im = convolution.to(\"cpu\").numpy()[0]\n",
    "\n",
    "    return new_im\n",
    "\n",
    "def pixel_dataset(Im_list: list[np.ndarray], filterbank: np.ndarray) -> list[np.ndarray]:\n",
    "    \"\"\"Funcion que aplica un banco de filtros a una lista de imagenes y retorna un arreglo con los pixeles de las imagenes filtradas\n",
    "\n",
    "    Args:\n",
    "        Im_list (list[numpy.ndarray]): Lista de imágenes de entrada RGB\n",
    "        filterbank (numpy.ndarray): Banco de filtros\n",
    "\n",
    "    Returns:\n",
    "        list[numpy.ndarray]: Lista con los pixeles de las imagenes filtradas\n",
    "    \"\"\"\n",
    "    pixels = [] #Lista con los pixeles de las imagenes filtradas\n",
    "    # YOUR CODE HERE\n",
    "    target_size = (256, 256)\n",
    "\n",
    "    for img in Im_list:\n",
    "        #Convertir la imagen a escala de grises\n",
    "        img = processing(img, target_size, color_space=\"gray\")\n",
    "\n",
    "        #Se obtiene el mapa de respuesta por medio del filterbank y empleando la funcion filter_map (H x W x N)\n",
    "        response_map = filter_map(img, filterbank)\n",
    "\n",
    "        #Se extrae los pixeles del mapa de respuesta, es decir, (H*W, N)\n",
    "        pixel_vectors = response_map.reshape(-1, response_map.shape[2])\n",
    "\n",
    "        #Se añade los pixeles a la lista pixels\n",
    "        pixels.extend(pixel_vectors)\n",
    "    return pixels\n",
    "\n",
    "def texture_map(Im: np.ndarray, filterbank: np.ndarray, texton_dict: KMeans) -> np.ndarray:\n",
    "    \"\"\"Funcion que mapea una imagen a textones\n",
    "\n",
    "    Args:\n",
    "        Im (numpy.ndarray): Imagen de entrada RGB\n",
    "        filterbank (numpy.ndarray): Banco de filtros\n",
    "        texton_dict (KMeans): Diccionario de textones\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Imagen mapeada a textones\n",
    "    \"\"\"\n",
    "    texture_im = '' # Imagen mapeada a textones\n",
    "    # YOUR CODE HERE\n",
    "    target_size = (256, 256)\n",
    "\n",
    "    #Convertir la imagen a escala de grises\n",
    "    Im = processing(Im, target_size, color_space=\"gray\")\n",
    "\n",
    "    #Se obtiene el mapa de respuesta por medio del filterbank y empleando la funcion filter_map (H x W x N)\n",
    "    response_map = filter_map(Im, filterbank)\n",
    "\n",
    "    H, W, N = response_map.shape\n",
    "\n",
    "    #Se aplana a (H*W, N)\n",
    "    pixels = response_map.reshape(-1, N)\n",
    "\n",
    "    #Predecir el label del cluster\n",
    "    labels = texton_dict.predict(pixels)\n",
    "\n",
    "    #Transformar la imagen de 2 dimensiones\n",
    "    texture_im = labels.reshape(H, W)\n",
    "    return texture_im\n",
    "\n",
    "\n",
    "Pixels=pixel_dataset(Im_list,filterbank)\n",
    "\n",
    "textones = 25 #cantidad de textones k\n",
    "model1='' #modelo K means entrenado\n",
    "# YOUR CODE HERE\n",
    "pixels_array = np.array(Pixels)\n",
    "\n",
    "#Se fija la semilla con un numero aleatorio, 30 en este caso, para obtener los mimsmos resultados de clustering\n",
    "# y se hace 10 iteraciones\n",
    "\n",
    "# Entrenamos con Kmeans\n",
    "model1 = KMeans(n_clusters=textones, random_state=30, n_init=10)\n",
    "model1.fit(pixels_array)\n",
    "\n",
    "def texture_histogram(Im: np.ndarray, filterbank: np.ndarray, texton_dict: KMeans, bins: int) -> np.ndarray:\n",
    "    \"\"\"Funcion que calcula el histograma de textones de una imagen\n",
    "\n",
    "    Args:\n",
    "        Im (numpy.ndarray): Imagen de entrada RGB\n",
    "        filterbank (numpy.ndarray): Banco de filtros\n",
    "        texton_dict (KMeans): Diccionario de textones\n",
    "        bins (int): Número de bins del histograma\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Histograma de textones normalizado\n",
    "    \"\"\"\n",
    "    histogram = \"\" # Histograma de textones normalizado\n",
    "    # YOUR CODE HERE\n",
    "    #Se emplea la funcion texture_map para obtener el mapa de texturas\n",
    "    texture_map_img = texture_map(Im, filterbank, texton_dict)\n",
    "\n",
    "    #se aplana para obtener el histograma\n",
    "    hist, _ = np.histogram(texture_map_img.flatten(), bins=bins, range=(0, bins))\n",
    "\n",
    "    #Se normaliza el histrograma y retorna este histograma\n",
    "    histogram = hist.astype(np.float32) / np.sum(hist)\n",
    "\n",
    "    return histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3209f82",
   "metadata": {},
   "source": [
    "### Modificacion de descriptores y extraccion de textones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050b757",
   "metadata": {},
   "source": [
    "Modificatemos los parametros para extraer los descriptores con mayor cantidad de información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b9bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Borrar antes de la experimentación\n",
    "del shape_des_train \n",
    "del color_des_train \n",
    "del shape_des_valid \n",
    "del color_des_valid \n",
    "del svm_color\n",
    "del svm_shape\n",
    "del proba_color_valid \n",
    "del proba_shape_valid\n",
    "del RF_color\n",
    "del RF_shape\n",
    "del MLP_color\n",
    "del MLP_shape\n",
    "del final_predict\n",
    "del color_predict\n",
    "del shape_predict\n",
    "del proba_color_valid_RF\n",
    "del proba_shape_valid_RF\n",
    "del proba_color_valid_MLP\n",
    "del proba_shape_valid_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c028313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para almacenar la información\n",
    "shape_des_train = []\n",
    "color_des_train = []\n",
    "textura_des_train = []\n",
    "textura_labels_train = []\n",
    "\n",
    "shape_des_valid = []\n",
    "color_des_valid = []\n",
    "textura_des_valid = []\n",
    "textura_labels_valid = []\n",
    "\n",
    "# Parametros\n",
    "resize_shape = (256, 256)\n",
    "patch_sizes = [(256, 256), (128, 128), (64, 64)]\n",
    "bins = 50 # Pasamos de 8 a 50 bins\n",
    "textones = 25 # Determinamos un alto número de textones\n",
    "hog_params = dict(\n",
    "    orientations=10,\n",
    "    pixels_per_cell=(8, 8),\n",
    "    cells_per_block=(2, 2),\n",
    "    visualize=False,\n",
    "    channel_axis=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer descriptores de entrenamiento\n",
    "print(\"Extrayendo descriptores de entrenamiento...\")\n",
    "for img_path in tqdm(train_paths, desc=\"Train\"):\n",
    "    img = cv2.imread(img_path)\n",
    "    img_rgb = processing(img, resize_shape, \"rgb\")\n",
    "    img_gray = processing(img, resize_shape,\"gray\")\n",
    "    \n",
    "    # Color descriptor\n",
    "    color_des_train.append(Color_pyramid(img_rgb, patch_sizes, \"concat\", bins))\n",
    "\n",
    "    # HOG descriptor \n",
    "    resized_hog = resize(img_rgb, (128, 64))\n",
    "    shape_des_train.append(hog(resized_hog, **hog_params))\n",
    "\n",
    "selected_train = list(zip(train_paths, train_labels))\n",
    "\n",
    "for img_path, label in tqdm(selected_train):\n",
    "    img = cv2.imread(img_path)\n",
    "    img_gray = processing(img, resize_shape,\"gray\")\n",
    "  \n",
    "    # Textura descriptor\n",
    "    hist = texture_histogram(img_gray, filterbank, model1, bins=textones)\n",
    "    textura_des_train.append(hist)\n",
    "    textura_labels_train.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc18fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer descriptores de validación\n",
    "print(\"Extrayendo descriptores de validación...\")\n",
    "for img_path in tqdm(valid_paths, desc=\"Validation\"):\n",
    "    img = cv2.imread(img_path)\n",
    "    img_rgb = processing(img, resize_shape, \"rgb\")\n",
    "    img_gray = processing(img, resize_shape,\"gray\")\n",
    "    \n",
    "    # Color descriptor\n",
    "    color_des_valid.append(Color_pyramid(img_rgb, patch_sizes, \"concat\", bins))\n",
    "\n",
    "    # HOG descriptor \n",
    "    resized_hog = resize(img_rgb, (128, 64))\n",
    "    shape_des_valid.append(hog(resized_hog, **hog_params))\n",
    "\n",
    "selected_valid = list(zip(valid_paths, valid_labels))\n",
    "\n",
    "for img_path, label in tqdm(selected_valid):\n",
    "    img = cv2.imread(img_path)\n",
    "    img_gray = processing(img, resize_shape,\"gray\")\n",
    "  \n",
    "    # Textura descriptor\n",
    "    hist = texture_histogram(img_gray, filterbank, model1, bins=textones)\n",
    "    textura_des_valid.append(hist)\n",
    "    textura_labels_valid.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Las dimensiones del descriptor de color son {np.array(color_des_train).shape[1]}\")\n",
    "print(f\"Las dimensiones del descriptor de forma son {np.array(shape_des_train).shape[1]}\")\n",
    "print(f\"Las dimensiones del descriptor de textura son {np.array(textura_des_train).shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5185a7",
   "metadata": {},
   "source": [
    "## Inicio de experimentaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e6f478",
   "metadata": {},
   "source": [
    "##### Experimentación de clasificación con descriptores multimodales con los mejores hiperparámetros estándar (Los hiperparámetros manejados durante la entrega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b46aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinación de descriptores con SVM\n",
    "combinacion_descriptores = [\n",
    "    (\"color + hog\", color_des_train, shape_des_train, color_des_valid, shape_des_valid),\n",
    "    (\"color + texture\", color_des_train, textura_des_train, color_des_valid, textura_des_valid),\n",
    "    (\"hog + texture\", shape_des_train, textura_des_train, shape_des_valid, textura_des_valid),\n",
    "    (\"color + hog + texture\",\n",
    "     [np.concatenate([c, h, t]) for c, h, t in zip(color_des_train, shape_des_train, textura_des_train)],\n",
    "     None,  \n",
    "     [np.concatenate([c, h, t]) for c, h, t in zip(color_des_valid, shape_des_valid, textura_des_valid)],\n",
    "     None)\n",
    "]\n",
    "\n",
    "for name, train1, train2, val1, val2 in combinacion_descriptores:\n",
    "    if train2 is None:  #En caso de que este ya concatenado\n",
    "        train_features = train1\n",
    "        val_features = val1\n",
    "    else:\n",
    "        train_features = [np.concatenate([a, b]) for a, b in zip(train1, train2)]\n",
    "        val_features = [np.concatenate([a, b]) for a, b in zip(val1, val2)]\n",
    "\n",
    "    model = make_pipeline(StandardScaler(),SVC(kernel='rbf', C=10.0,probability=True))\n",
    "    model.fit(train_features, train_labels)\n",
    "\n",
    "    predictions = model.predict_proba(val_features)\n",
    "    \n",
    "    final_preediccion = np.argmax(predictions,axis=1)\n",
    "\n",
    "    f_score = f1_score(valid_labels, final_preediccion, average='macro')\n",
    "    print(f\"F-score for {name} model: {f_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec245e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinación de descriptores con RF y MLP\n",
    "combinacion_descriptores = [\n",
    "    (\"color + hog\", color_des_train, shape_des_train, color_des_valid, shape_des_valid),\n",
    "    (\"color + texture\", color_des_train, textura_des_train, color_des_valid, textura_des_valid),\n",
    "    (\"hog + texture\", shape_des_train, textura_des_train, shape_des_valid, textura_des_valid),\n",
    "    (\"color + hog + texture\",\n",
    "     [np.concatenate([c, h, t]) for c, h, t in zip(color_des_train, shape_des_train, textura_des_train)],\n",
    "     None,  \n",
    "     [np.concatenate([c, h, t]) for c, h, t in zip(color_des_valid, shape_des_valid, textura_des_valid)],\n",
    "     None)\n",
    "]\n",
    "\n",
    "for name, train1, train2, val1, val2 in combinacion_descriptores:\n",
    "    if train2 is None:  #En caso de que este ya concatenado\n",
    "        train_features = train1\n",
    "        val_features = val1\n",
    "    else:\n",
    "        train_features = [np.concatenate([a, b]) for a, b in zip(train1, train2)]\n",
    "        val_features = [np.concatenate([a, b]) for a, b in zip(val1, val2)]    \n",
    "    \n",
    "    Model_RF = RandomForestClassifier(n_estimators=100, random_state=30)\n",
    "    Model_RF.fit(train_features, train_labels)\n",
    "    \n",
    "    predictions_RF = Model_RF.predict(val_features)\n",
    "    f_score_RF = f1_score(valid_labels, predictions_RF, average='macro')\n",
    "\n",
    "    print(f\"Para el clasificador RF se obtiene:\")\n",
    "    print(f\"El F-score para la combinación de descriptores {name} es: {f_score_RF:.4f}\")\n",
    "    \n",
    "    # Modelos de MLP\n",
    "    MLP_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=30)\n",
    "    MLP_model.fit(train_features, train_labels)\n",
    "    predictions_MLP = MLP_model.predict(val_features)\n",
    "    f_score_MLP = f1_score(valid_labels, predictions_MLP, average='macro')\n",
    "\n",
    "    print(f\"Para el clasificador MLP se obtiene:\")\n",
    "    print(f\"El F-score para la combinación de descriptores {name} es: {f_score_MLP:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8447d6c6",
   "metadata": {},
   "source": [
    "Se observa que el mejor F1 obtenido por color + hog + texture modelo: 0.6972 supera el baselinea de 0.66542"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d69497",
   "metadata": {},
   "source": [
    "##### 2. Experimentación modelos expertos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4c58c",
   "metadata": {},
   "source": [
    "##### Experimento 1 - Hiperparámetros manejados durante toda la entrega\n",
    "*Parámetros:*\n",
    "\n",
    "RF: n_estimators=100, random_state=30\n",
    "\n",
    "MLP: hidden_layer_sizes=(100,), max_iter=1000, random_state=30\n",
    "\n",
    "SVM: random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptor de color \n",
    "\n",
    "# Modelo RF\n",
    "RF_color = RandomForestClassifier(n_estimators=100, random_state=30)\n",
    "RF_color.fit(color_des_train, train_labels)\n",
    "\n",
    "# Modelo MLP\n",
    "MLP_color = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=30)\n",
    "MLP_color.fit(color_des_train, train_labels)\n",
    "\n",
    "# Modelo SVM\n",
    "SVM_color = make_pipeline(StandardScaler(), SVC(probability=True, random_state=42))\n",
    "SVM_color.fit(color_des_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8363d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptor de shape\n",
    "\n",
    "# Modelo RF\n",
    "RF_shape = RandomForestClassifier(n_estimators=100, random_state=30)\n",
    "RF_shape.fit(shape_des_train, train_labels)\n",
    "\n",
    "# Modelo MLP\n",
    "MLP_shape = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=30)\n",
    "MLP_shape.fit(shape_des_train, train_labels)\n",
    "\n",
    "# Modelo SVM\n",
    "SVM_shape = make_pipeline(StandardScaler(), SVC(probability=True, random_state=42))\n",
    "SVM_shape.fit(shape_des_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea19db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptor de textura\n",
    "\n",
    "# Modelo RF\n",
    "RF_textura = RandomForestClassifier(n_estimators=100, random_state=30)\n",
    "RF_textura.fit(textura_des_train, train_labels)\n",
    "\n",
    "# Modelo MLP\n",
    "MLP_textura = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=30)\n",
    "MLP_textura.fit(textura_des_train, train_labels)\n",
    "\n",
    "# Modelo SVM\n",
    "SVM_textura = make_pipeline(StandardScaler(), SVC(probability=True, random_state=42))\n",
    "SVM_textura.fit(textura_des_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3c1ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelos\n",
    "modelo_RF = [RF_color, RF_shape, RF_textura]\n",
    "modelo_MLP = [MLP_color, MLP_shape, MLP_textura]\n",
    "modelo_SVM = [SVM_color, SVM_shape, SVM_textura]\n",
    "\n",
    "# Inputs\n",
    "RF_inputs = [color_des_valid, shape_des_valid, textura_des_valid]\n",
    "MLP_inputs = [color_des_valid, shape_des_valid, textura_des_valid]\n",
    "SVM_inputs = [color_des_valid, shape_des_valid, textura_des_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d51345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción de probabilidades para SVM RF y MLP \n",
    "proba_color_valid_RF, proba_shape_valid_RF, proba_textura_valid_RF = [model.predict_proba(data) for model, data in zip(modelo_RF, RF_inputs)]\n",
    "proba_color_valid_MLP, proba_shape_valid_MLP, proba_textura_valid_MLP = [model.predict_proba(data) for model, data in zip(modelo_MLP, MLP_inputs)]\n",
    "proba_color_valid_SVM, proba_shape_valid_SVM, proba_textura_valid_SVM = [model.predict_proba(data) for model, data in zip(modelo_SVM, SVM_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones\n",
    "color_predict_RF, shape_predict_RF, textura_predict_RF = [model.predict(data) for model, data in zip(modelo_RF, RF_inputs)]\n",
    "color_predict_MLP, shape_predict_MLP, textura_predict_MLP = [model.predict(data) for model, data in zip(modelo_MLP, MLP_inputs)]\n",
    "color_predict_SVM, shape_predict_SVM, textura_predict_SVM = [model.predict(data) for model, data in zip(modelo_SVM, SVM_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos expertos\n",
    "avg_proba_RF = (proba_color_valid_RF + proba_shape_valid_RF + proba_textura_valid_RF) / 3\n",
    "avg_proba_MLP = (proba_color_valid_MLP + proba_shape_valid_MLP + proba_textura_valid_MLP) / 3\n",
    "avg_proba_SVM = (proba_color_valid_SVM + proba_shape_valid_SVM + proba_textura_valid_SVM) / 3\n",
    "\n",
    "experto_RF_predict = np.argmax(avg_proba_RF, axis=1)\n",
    "experto_MLP_predict = np.argmax(avg_proba_MLP, axis=1)\n",
    "experto_SVM_predict = np.argmax(avg_proba_SVM, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6652f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score\n",
    "\n",
    "# Modelo RF\n",
    "f_color_RF=f1_score(valid_labels,color_predict_RF,average='macro')\n",
    "f_shape_RF=f1_score(valid_labels,shape_predict_RF,average='macro')\n",
    "f_textura_RF=f1_score(valid_labels,textura_predict_RF,average='macro')\n",
    "f_experto_RF=f1_score(valid_labels,experto_RF_predict,average='macro')\n",
    "\n",
    "# Modelo MLP\n",
    "f_color_MLP=f1_score(valid_labels,color_predict_MLP,average='macro')\n",
    "f_shape_MLP=f1_score(valid_labels,shape_predict_MLP,average='macro')\n",
    "f_textura_MLP=f1_score(valid_labels,textura_predict_MLP,average='macro')\n",
    "f_experto_MLP=f1_score(valid_labels,experto_MLP_predict,average='macro')\n",
    "\n",
    "# Modelo SVM\n",
    "f_color_SVM=f1_score(valid_labels,color_predict_SVM,average='macro')\n",
    "f_shape_SVM=f1_score(valid_labels,shape_predict_SVM,average='macro')\n",
    "f_textura_SVM=f1_score(valid_labels,textura_predict_SVM,average='macro')\n",
    "f_experto_SVM=f1_score(valid_labels,experto_SVM_predict,average='macro')\n",
    "\n",
    "print('Random Forest')\n",
    "print(f'color: {f_color_RF}')\n",
    "print(f'forma: {f_shape_RF}')\n",
    "print(f'textura: {f_textura_RF}')\n",
    "print(f'final: {f_experto_RF}')\n",
    "print('MLP')\n",
    "print(f'color: {f_color_MLP}')\n",
    "print(f'forma: {f_shape_MLP}')\n",
    "print(f'textura: {f_textura_MLP}')\n",
    "print(f'final: {f_experto_MLP}')\n",
    "print('MLP')\n",
    "print(f'color: {f_color_SVM}')\n",
    "print(f'forma: {f_shape_SVM}')\n",
    "print(f'textura: {f_textura_SVM}')\n",
    "print(f'final: {f_experto_SVM}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a0c0e",
   "metadata": {},
   "source": [
    "##### Experimento 2 - Hiperparámetros escogidos\n",
    "*Parámetros:*\n",
    "\n",
    "RF: n_estimators=200, max_depth=10, random_state=42\n",
    "\n",
    "MLP: hidden_layer_sizes=(150,50), max_iter=1500, alpha =0.001, random_state=42\n",
    "\n",
    "SVM: random_state=42, kernel=\"poly\", degree=4, c=5, gamma=\"scale\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea878304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptor de color \n",
    "\n",
    "# Modelo RF\n",
    "RF_color = RandomForestClassifier(n_estimators=200, random_state=42,max_depth=10)\n",
    "RF_color.fit(color_des_train, train_labels)\n",
    "\n",
    "# Modelo MLP\n",
    "MLP_color = MLPClassifier( hidden_layer_sizes=(150,50), max_iter=1500, alpha =0.001, random_state=42)\n",
    "MLP_color.fit(color_des_train, train_labels)\n",
    "\n",
    "# Modelo SVM\n",
    "SVM_color = make_pipeline(StandardScaler(), SVC(probability=True,random_state=42, kernel=\"poly\", degree=4, C=5, gamma=\"scale\"))\n",
    "SVM_color.fit(color_des_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f7feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptor de shape\n",
    "\n",
    "# Modelo RF\n",
    "RF_shape = RandomForestClassifier(n_estimators=200, random_state=42,max_depth=10)\n",
    "RF_shape.fit(shape_des_train, train_labels)\n",
    "\n",
    "# Modelo MLP\n",
    "MLP_shape = MLPClassifier( hidden_layer_sizes=(150,50), max_iter=1500, alpha =0.001, random_state=42)\n",
    "MLP_shape.fit(shape_des_train, train_labels)\n",
    "\n",
    "# Modelo SVM\n",
    "SVM_shape = make_pipeline(StandardScaler(), SVC(probability=True,random_state=42, kernel=\"poly\", degree=4, C=5, gamma=\"scale\"))\n",
    "SVM_shape.fit(shape_des_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3cdfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptor de textura\n",
    "\n",
    "# Modelo RF\n",
    "RF_textura = RandomForestClassifier(n_estimators=200, random_state=42,max_depth=10)\n",
    "RF_textura.fit(textura_des_train, train_labels)\n",
    "\n",
    "# Modelo MLP\n",
    "MLP_textura = MLPClassifier( hidden_layer_sizes=(150,50), max_iter=1500, alpha =0.001, random_state=42)\n",
    "MLP_textura.fit(textura_des_train, train_labels)\n",
    "\n",
    "# Modelo SVM\n",
    "SVM_textura = make_pipeline(StandardScaler(), SVC(probability=True,random_state=42, kernel=\"poly\", degree=4, C=5, gamma=\"scale\"))\n",
    "SVM_textura.fit(textura_des_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea693feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelos\n",
    "modelo_RF = [RF_color, RF_shape, RF_textura]\n",
    "modelo_MLP = [MLP_color, MLP_shape, MLP_textura]\n",
    "modelo_SVM = [SVM_color, SVM_shape, SVM_textura]\n",
    "\n",
    "# Inputs\n",
    "RF_inputs = [color_des_valid, shape_des_valid, textura_des_valid]\n",
    "MLP_inputs = [color_des_valid, shape_des_valid, textura_des_valid]\n",
    "SVM_inputs = [color_des_valid, shape_des_valid, textura_des_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee3781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción de probabilidades para SVM RF y MLP \n",
    "proba_color_valid_RF, proba_shape_valid_RF, proba_textura_RF = [model.predict_proba(data) for model, data in zip(modelo_RF, RF_inputs)]\n",
    "proba_color_valid_MLP, proba_shape_valid_MLP, proba_textura_MLP = [model.predict_proba(data) for model, data in zip(modelo_MLP, MLP_inputs)]\n",
    "proba_color_valid_SVM, proba_shape_valid_SVM, proba_textura_SVM = [model.predict_proba(data) for model, data in zip(modelo_SVM, SVM_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a8edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones\n",
    "color_predict_RF, shape_predict_RF, textura_predict_RF = [model.predict(data) for model, data in zip(modelo_RF, RF_inputs)]\n",
    "color_predict_MLP, shape_predict_MLP, textura_predict_MLP = [model.predict(data) for model, data in zip(modelo_MLP, MLP_inputs)]\n",
    "color_predict_SVM, shape_predict_SVM, textura_predict_SVM = [model.predict(data) for model, data in zip(modelo_SVM, SVM_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59157a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos expertos\n",
    "avg_proba_RF = (proba_color_valid_RF + proba_shape_valid_RF + proba_textura_RF) / 3\n",
    "avg_proba_MLP = (proba_color_valid_MLP + proba_shape_valid_MLP + proba_textura_MLP) / 3\n",
    "avg_proba_SVM = (proba_color_valid_SVM + proba_shape_valid_SVM + proba_textura_SVM) / 3\n",
    "\n",
    "experto_RF_predict = np.argmax(avg_proba_RF, axis=1)\n",
    "experto_MLP_predict = np.argmax(avg_proba_MLP, axis=1)\n",
    "experto_SVM_predict = np.argmax(avg_proba_SVM, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82decf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score\n",
    "\n",
    "# Modelo RF\n",
    "f_color_RF=f1_score(valid_labels,color_predict_RF,average='macro')\n",
    "f_shape_RF=f1_score(valid_labels,shape_predict_RF,average='macro')\n",
    "f_textura_RF=f1_score(valid_labels,textura_predict_RF,average='macro')\n",
    "f_experto_RF=f1_score(valid_labels,experto_RF_predict,average='macro')\n",
    "\n",
    "# Modelo MLP\n",
    "f_color_MLP=f1_score(valid_labels,color_predict_MLP,average='macro')\n",
    "f_shape_MLP=f1_score(valid_labels,shape_predict_MLP,average='macro')\n",
    "f_textura_MLP=f1_score(valid_labels,textura_predict_MLP,average='macro')\n",
    "f_experto_MLP=f1_score(valid_labels,experto_MLP_predict,average='macro')\n",
    "\n",
    "# Modelo SVM\n",
    "f_color_SVM=f1_score(valid_labels,color_predict_SVM,average='macro')\n",
    "f_shape_SVM=f1_score(valid_labels,shape_predict_SVM,average='macro')\n",
    "f_textura_SVM=f1_score(valid_labels,textura_predict_SVM,average='macro')\n",
    "f_experto_SVM=f1_score(valid_labels,experto_SVM_predict,average='macro')\n",
    "\n",
    "print('Random Forest')\n",
    "print(f'color: {f_color_RF}')\n",
    "print(f'forma: {f_shape_RF}')\n",
    "print(f'textura: {f_textura_RF}')\n",
    "print(f'final: {f_experto_RF}')\n",
    "print('MLP')\n",
    "print(f'color: {f_color_MLP}')\n",
    "print(f'forma: {f_shape_MLP}')\n",
    "print(f'textura: {f_textura_MLP}')\n",
    "print(f'final: {f_experto_MLP}')\n",
    "print('SVM')\n",
    "print(f'color: {f_color_SVM}')\n",
    "print(f'forma: {f_shape_SVM}')\n",
    "print(f'textura: {f_textura_SVM}')\n",
    "print(f'final: {f_experto_SVM}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db3223",
   "metadata": {},
   "source": [
    "##### Experimento 3 - Kernels alternativos y sin regulación\n",
    "*Parámetros:*\n",
    "\n",
    "RF: n_estimators=200, max_depth=None, random_state=0\n",
    "\n",
    "MLP: hidden_layer_sizes=(50,), max_iter=1500, random_state=0\n",
    "\n",
    "SVM: random_state=0, kernel=\"sigmoid\", c=1, gamma=\"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34712930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptor de color \n",
    "\n",
    "# Modelo RF\n",
    "RF_color = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=0)\n",
    "RF_color.fit(color_des_train, train_labels)\n",
    "\n",
    "# Modelo MLP\n",
    "MLP_color = MLPClassifier( hidden_layer_sizes=(50,), max_iter=1500, random_state=0)\n",
    "MLP_color.fit(color_des_train, train_labels)\n",
    "\n",
    "# Modelo SVM\n",
    "SVM_color = make_pipeline(StandardScaler(), SVC(probability=True,random_state=0, kernel=\"sigmoid\", C=1, gamma=\"auto\"))\n",
    "SVM_color.fit(color_des_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb78ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptor de shape\n",
    "\n",
    "# Modelo RF\n",
    "RF_shape = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=0)\n",
    "RF_shape.fit(shape_des_train, train_labels)\n",
    "\n",
    "# Modelo MLP\n",
    "MLP_shape = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1500, random_state=0)\n",
    "MLP_shape.fit(shape_des_train, train_labels)\n",
    "\n",
    "# Modelo SVM\n",
    "SVM_shape = make_pipeline(StandardScaler(), SVC(probability=True,random_state=0, kernel=\"sigmoid\", C=1, gamma=\"auto\"))\n",
    "SVM_shape.fit(shape_des_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94557e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptor de textura\n",
    "\n",
    "# Modelo RF\n",
    "RF_textura = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=0)\n",
    "RF_textura.fit(textura_des_train, train_labels)\n",
    "\n",
    "# Modelo MLP\n",
    "MLP_textura = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1500, random_state=0)\n",
    "MLP_textura.fit(textura_des_train, train_labels)\n",
    "\n",
    "# Modelo SVM\n",
    "SVM_textura = make_pipeline(StandardScaler(), SVC(probability=True,random_state=0, kernel=\"sigmoid\", C=1, gamma=\"auto\"))\n",
    "SVM_textura.fit(textura_des_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb94db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelos\n",
    "modelo_RF = [RF_color, RF_shape, RF_textura]\n",
    "modelo_MLP = [MLP_color, MLP_shape, MLP_textura]\n",
    "modelo_SVM = [SVM_color, SVM_shape, SVM_textura]\n",
    "\n",
    "# Inputs\n",
    "RF_inputs = [color_des_valid, shape_des_valid, textura_des_valid]\n",
    "MLP_inputs = [color_des_valid, shape_des_valid, textura_des_valid]\n",
    "SVM_inputs = [color_des_valid, shape_des_valid, textura_des_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651fa749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción de probabilidades para SVM RF y MLP \n",
    "proba_color_valid_RF, proba_shape_valid_RF, proba_textura_RF = [model.predict_proba(data) for model, data in zip(modelo_RF, RF_inputs)]\n",
    "proba_color_valid_MLP, proba_shape_valid_MLP, proba_textura_MLP = [model.predict_proba(data) for model, data in zip(modelo_MLP, MLP_inputs)]\n",
    "proba_color_valid_SVM, proba_shape_valid_SVM, proba_textura_SVM = [model.predict_proba(data) for model, data in zip(modelo_SVM, SVM_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab3fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones\n",
    "color_predict_RF, shape_predict_RF, textura_predict_RF = [model.predict(data) for model, data in zip(modelo_RF, RF_inputs)]\n",
    "color_predict_MLP, shape_predict_MLP, textura_predict_MLP = [model.predict(data) for model, data in zip(modelo_MLP, MLP_inputs)]\n",
    "color_predict_SVM, shape_predict_SVM, textura_predict_SVM = [model.predict(data) for model, data in zip(modelo_SVM, SVM_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos expertos\n",
    "avg_proba_RF = (proba_color_valid_RF + proba_shape_valid_RF + proba_textura_RF) / 3\n",
    "avg_proba_MLP = (proba_color_valid_MLP + proba_shape_valid_MLP + proba_textura_MLP) / 3\n",
    "avg_proba_SVM = (proba_color_valid_SVM + proba_shape_valid_SVM + proba_textura_SVM) / 3\n",
    "\n",
    "experto_RF_predict = np.argmax(avg_proba_RF, axis=1)\n",
    "experto_MLP_predict = np.argmax(avg_proba_MLP, axis=1)\n",
    "experto_SVM_predict = np.argmax(avg_proba_SVM, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b6fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score\n",
    "\n",
    "# Modelo RF\n",
    "f_color_RF=f1_score(valid_labels,color_predict_RF,average='macro')\n",
    "f_shape_RF=f1_score(valid_labels,shape_predict_RF,average='macro')\n",
    "f_textura_RF=f1_score(valid_labels,textura_predict_RF,average='macro')\n",
    "f_experto_RF=f1_score(valid_labels,experto_RF_predict,average='macro')\n",
    "\n",
    "# Modelo MLP\n",
    "f_color_MLP=f1_score(valid_labels,color_predict_MLP,average='macro')\n",
    "f_shape_MLP=f1_score(valid_labels,shape_predict_MLP,average='macro')\n",
    "f_textura_MLP=f1_score(valid_labels,textura_predict_MLP,average='macro')\n",
    "f_experto_MLP=f1_score(valid_labels,experto_MLP_predict,average='macro')\n",
    "\n",
    "# Modelo SVM\n",
    "f_color_SVM=f1_score(valid_labels,color_predict_SVM,average='macro')\n",
    "f_shape_SVM=f1_score(valid_labels,shape_predict_SVM,average='macro')\n",
    "f_textura_SVM=f1_score(valid_labels,textura_predict_SVM,average='macro')\n",
    "f_experto_SVM=f1_score(valid_labels,experto_SVM_predict,average='macro')\n",
    "\n",
    "print('Random Forest')\n",
    "print(f'color: {f_color_RF}')\n",
    "print(f'forma: {f_shape_RF}')\n",
    "print(f'textura: {f_textura_RF}')\n",
    "print(f'final: {f_experto_RF}')\n",
    "print('MLP')\n",
    "print(f'color: {f_color_MLP}')\n",
    "print(f'forma: {f_shape_MLP}')\n",
    "print(f'textura: {f_textura_MLP}')\n",
    "print(f'final: {f_experto_MLP}')\n",
    "print('MLP')\n",
    "print(f'color: {f_color_SVM}')\n",
    "print(f'forma: {f_shape_SVM}')\n",
    "print(f'textura: {f_textura_SVM}')\n",
    "print(f'final: {f_experto_SVM}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846459fe",
   "metadata": {},
   "source": [
    "##### Experimentación 4 Creación de nuevas imagenes para el entrenaiento "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc26efa9",
   "metadata": {},
   "source": [
    "###### Data Augmentation( Creación de nuevos datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874ff3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "def data_augmentation(Im: np.ndarray, transformations: list[str],\n",
    "                      alpha_factor: tuple[float] = (0.8, 1.2),\n",
    "                      beta_factor: tuple[int] = (-30, 30)) -> np.ndarray:\n",
    "    augmented = Im.copy()\n",
    "\n",
    "    # Horizontal Flip\n",
    "    if \"flip\" in transformations:\n",
    "        augmented = cv2.flip(augmented, 1)\n",
    "\n",
    "    # Brightness & Contrast\n",
    "    if \"brightness and contrast\" in transformations:\n",
    "        alpha = random.uniform(*alpha_factor)\n",
    "        beta = random.uniform(*beta_factor)\n",
    "        augmented = cv2.convertScaleAbs(augmented, alpha=alpha, beta=beta)\n",
    "\n",
    "    elif \"contrast\" in transformations:\n",
    "        beta = random.uniform(*beta_factor)\n",
    "        augmented = cv2.convertScaleAbs(augmented, alpha=1, beta=beta)\n",
    "\n",
    "    elif \"brightness\" in transformations:\n",
    "        alpha = random.uniform(*alpha_factor)\n",
    "        augmented = cv2.convertScaleAbs(augmented, alpha=alpha, beta=0)\n",
    "\n",
    "    # baja nitidez \n",
    "    if \"blur\" in transformations:\n",
    "        ksize = random.choice([3, 5])\n",
    "        augmented = cv2.GaussianBlur(augmented, (ksize, ksize), 0)\n",
    "\n",
    "    # rotación\n",
    "    if \"rotate\" in transformations:\n",
    "        angle = random.uniform(-15, 15)\n",
    "        h, w = augmented.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1)\n",
    "        augmented = cv2.warpAffine(augmented, M, (w, h))\n",
    "\n",
    "    # escalar\n",
    "    if \"scale\" in transformations:\n",
    "        scale_factor = random.uniform(0.9, 1.1)\n",
    "        augmented = cv2.resize(augmented, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        augmented = cv2.resize(augmented, (Im.shape[1], Im.shape[0]))\n",
    "\n",
    "    # Translation\n",
    "    if \"translate\" in transformations:\n",
    "        tx = random.randint(-10, 10)\n",
    "        ty = random.randint(-10, 10)\n",
    "        M = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "        augmented = cv2.warpAffine(augmented, M, (Im.shape[1], Im.shape[0]))\n",
    "\n",
    "    # ruido\n",
    "    if \"noise\" in transformations:\n",
    "        noise = np.random.normal(0, 10, augmented.shape).astype(np.uint8)\n",
    "        augmented = cv2.add(augmented, noise)\n",
    "\n",
    "    # quitar el ruido\n",
    "    if \"denoise\" in transformations:\n",
    "        augmented = cv2.fastNlMeansDenoisingColored(augmented, None, 10, 10, 7, 21)\n",
    "\n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bfb3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#Cargar 2 imagenes\n",
    "img_paths = [train_paths[0], train_paths[1]]\n",
    "images_rgb = [cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2RGB) for p in img_paths]\n",
    "\n",
    "#Aplicar filtros\n",
    "augmentations = [\n",
    "    \"flip\",\n",
    "    \"brightness and contrast\",\n",
    "    \"blur\",\n",
    "    \"rotate\",\n",
    "    \"scale\",\n",
    "    \"translate\",\n",
    "    \"noise\",\n",
    "    \"denoise\"\n",
    "]\n",
    "\n",
    "n_rows = len(augmentations) + 1\n",
    "n_cols = 2\n",
    "\n",
    "plt.figure(figsize=(10, 18))\n",
    "for i, img_rgb in enumerate(images_rgb):\n",
    "    # Original \n",
    "    plt.subplot(n_rows, n_cols, 1 + i)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(f\"Original {i + 1}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Aplicar augmentation\n",
    "    for j, aug in enumerate(augmentations):\n",
    "        aug_img = data_augmentation(\n",
    "            img_rgb,\n",
    "            transformations=[aug],\n",
    "            alpha_factor=(0.9, 1.1),\n",
    "            beta_factor=(-30, 30)\n",
    "        )\n",
    "        plt.subplot(n_rows, n_cols, (j + 1) * 2 + i + 1)\n",
    "        plt.imshow(aug_img)\n",
    "        plt.title(f\"{aug.title()} {i + 1}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713f4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Parametros\n",
    "resize_shape = (256, 256)\n",
    "patch_sizes = [(256, 256), (128, 128), (64, 64)]\n",
    "bins = 50\n",
    "hog_params = dict(\n",
    "    orientations=10,\n",
    "    pixels_per_cell=(8, 8),\n",
    "    cells_per_block=(2, 2),\n",
    "    visualize=False,\n",
    "    channel_axis=-1\n",
    ")\n",
    "\n",
    "# Almacenar listas\n",
    "aug_color_des_train = []\n",
    "aug_shape_des_train = []\n",
    "aug_texture_des_train = []\n",
    "aug_train_labels = []\n",
    "\n",
    "# Maximo numera de muestras\n",
    "max_augmented = 5163\n",
    "augmented_so_far = 0\n",
    "\n",
    "# === Loop ===\n",
    "for img_path, label in tqdm(zip(train_paths, train_labels), total=len(train_paths), desc=\"Aumento del dataset\"):\n",
    "    if augmented_so_far >= max_augmented:\n",
    "        break\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        continue\n",
    "\n",
    "    img_rgb = processing(img, resize_shape, \"rgb\")\n",
    "\n",
    "    # === Descriptores para la imagen Original===\n",
    "    color_desc = Color_pyramid(img_rgb, patch_sizes, \"concat\", bins)\n",
    "    hog_desc = hog(resize(img_rgb, (128, 64)), **hog_params)\n",
    "    texture_desc = texture_histogram(img_rgb,filterbank,model1,bins=25)\n",
    "\n",
    "    aug_color_des_train.append(color_desc)\n",
    "    aug_shape_des_train.append(hog_desc)\n",
    "    aug_texture_des_train.append(texture_desc)\n",
    "    aug_train_labels.append(label)\n",
    "\n",
    "    # === generar imagen ===\n",
    "    aug_img = data_augmentation(\n",
    "        img_rgb,\n",
    "        transformations=[\"flip\", \"brightness and contrast\", \"denoise\", \"contrast\", \"blur\", \"rotate\", \"scale\", \"translate\"],\n",
    "        alpha_factor=(0.9, 1.1),\n",
    "        beta_factor=(-30, 30)\n",
    "    )\n",
    "\n",
    "    # === Descriptores con las nuevas Imagagenes ===\n",
    "    color_desc_aug = Color_pyramid(aug_img, patch_sizes, \"concat\", bins)\n",
    "    hog_desc_aug = hog(resize(aug_img, (128, 64)), **hog_params)\n",
    "    texture_desc_aug =texture_histogram(aug_img,filterbank,model1,bins=25)\n",
    "\n",
    "    aug_color_des_train.append(color_desc_aug)\n",
    "    aug_shape_des_train.append(hog_desc_aug)\n",
    "    aug_train_labels.append(label)\n",
    "    aug_texture_des_train.append(texture_desc)\n",
    "\n",
    "    # Contador\n",
    "    augmented_so_far += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5dc991",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(aug_shape_des_train))\n",
    "print(len(aug_color_des_train))\n",
    "print(len(aug_texture_des_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04aeb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Hiperparametros ===\n",
    "kernels = ['linear', 'rbf']\n",
    "C_values = [1.0, 10.0]\n",
    "\n",
    "# === Entrenar SVMs para COLOR (pyramid descriptor) ===\n",
    "print(\"Entrenando SVMs con descriptores de COLOR (piramide)...\")\n",
    "\n",
    "\n",
    "svm_kernel2_C1_color = make_pipeline(StandardScaler(),SVC(probability=True,kernel=kernels[1], C=C_values[0]))\n",
    "svm_kernel2_C1_color.fit(aug_color_des_train, aug_train_labels)\n",
    "\n",
    "svm_kernel2_C2_color = make_pipeline(StandardScaler(),SVC(probability=True,kernel=kernels[1], C=C_values[1]))\n",
    "svm_kernel2_C2_color.fit(aug_color_des_train, aug_train_labels)\n",
    "\n",
    "# === Entrenar SVMs para forma (HOG) ===\n",
    "print(\"Entrenando SVMs con descriptores de FORMA (HOG)...\")\n",
    "\n",
    "\n",
    "svm_kernel2_C1_forma = make_pipeline(StandardScaler(),SVC(probability=True,kernel=kernels[1], C=C_values[0]))\n",
    "svm_kernel2_C1_forma.fit(aug_shape_des_train, aug_train_labels)\n",
    "\n",
    "\n",
    "svm_kernel2_C2_forma = make_pipeline(StandardScaler(),SVC(probability=True,kernel=kernels[1], C=C_values[1]))\n",
    "svm_kernel2_C2_forma.fit(aug_shape_des_train, aug_train_labels)\n",
    "\n",
    "# === Entrenar SVMs para Textura ===\n",
    "print(\"Entrenando SVMs con descriptores de TEXTURA (textones)...\")\n",
    "\n",
    "svm_kernel2_C1_textura = make_pipeline(StandardScaler(),SVC(probability=True,kernel=kernels[1], C=C_values[0]))\n",
    "svm_kernel2_C1_textura.fit(aug_texture_des_train, aug_train_labels)\n",
    "\n",
    "svm_kernel2_C2_textura = make_pipeline(StandardScaler(),SVC(probability=True,kernel=kernels[1], C=C_values[1]))\n",
    "svm_kernel2_C2_textura.fit(aug_texture_des_train, aug_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec24f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# === PREDICTIONS ===\n",
    "final_predict1=[] #predicción final por expertos\n",
    "color_predict1=[] #predicción final por color\n",
    "shape_predict1=[] #predicción final por forma\n",
    "final_predict2=[] #predicción final por expertos\n",
    "color_predict2=[] #predicción final por color\n",
    "shape_predict2=[] #predicción final por forma\n",
    "\n",
    "# Color Pyramid\n",
    "predict_svm_kernel2_C1_color = svm_kernel2_C1_color.predict_proba(color_des_valid)\n",
    "predict_svm_kernel2_C2_color = svm_kernel2_C2_color.predict_proba(color_des_valid)\n",
    "\n",
    "# HOG\n",
    "predict_svm_kernel2_C1_hog = svm_kernel2_C1_forma.predict_proba(shape_des_valid)\n",
    "predict_svm_kernel2_C2_hog = svm_kernel2_C2_forma.predict_proba(shape_des_valid)\n",
    "\n",
    "# TEXTURA\n",
    "predict_svm_kernel2_C1_textura = svm_kernel2_C1_textura.predict_proba(textura_des_valid)\n",
    "predict_svm_kernel2_C2_textura = svm_kernel2_C2_textura.predict_proba(textura_des_valid)\n",
    "\n",
    "color_predict1 = np.argmax(predict_svm_kernel2_C1_color, axis=1)\n",
    "shape_predict1 = np.argmax(predict_svm_kernel2_C1_hog, axis=1)\n",
    "textura_predict1 = np.argmax(predict_svm_kernel2_C1_textura, axis=1)\n",
    "\n",
    "# Promedio de las probabilidades\n",
    "average_proba1 = (color_predict1 + shape_predict1+ textura_predict1 ) / 3\n",
    "\n",
    "# Predicción final basada en el promedio de probabilidades\n",
    "final_predict1 = np.argmax(average_proba1, axis=1)\n",
    "\n",
    "f_color1=f1_score(valid_labels,color_predict1,average='macro')\n",
    "f_shape1=f1_score(valid_labels,shape_predict1,average='macro')\n",
    "f_textura1=f1_score(valid_labels,textura_predict1,average='macro')\n",
    "\n",
    "f_final1=f1_score(valid_labels,final_predict1,average='macro')\n",
    "\n",
    "print(f'color: {f_color1}')\n",
    "print(f'forma: {f_shape1}')\n",
    "print(f'textura: {f_textura1}')\n",
    "print(f'final: {f_final1}')\n",
    "\n",
    "# === F1 EVALUACION PUNTAJE ===\n",
    "\n",
    "color_predict2 = np.argmax(predict_svm_kernel2_C2_color, axis=1)\n",
    "shape_predict2 = np.argmax(predict_svm_kernel2_C2_hog, axis=1)\n",
    "textura_predict2 = np.argmax(predict_svm_kernel2_C2_textura, axis=1)\n",
    "\n",
    "# Promedio de las probabilidades\n",
    "average_proba2 = (color_predict2 + shape_predict2+ textura_predict2 ) / 3\n",
    "\n",
    "# Predicción final basada en el promedio de probabilidades\n",
    "final_predict2 = np.argmax(average_proba2, axis=1)\n",
    "\n",
    "f_color2=f1_score(valid_labels,color_predict2,average='macro')\n",
    "f_shape2=f1_score(valid_labels,shape_predict2,average='macro')\n",
    "f_textura2=f1_score(valid_labels,textura_predict2,average='macro')\n",
    "\n",
    "f_final2=f1_score(valid_labels,final_predict2,average='macro')\n",
    "\n",
    "print(f'color: {f_color2}')\n",
    "print(f'forma: {f_shape2}')\n",
    "print(f'textura: {f_textura2}')\n",
    "print(f'final: {f_final2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d91202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# === Combinaciones con dataset AUMENTADO para entrenamiento ===\n",
    "combinacion_descriptores = [\n",
    "    (\"color + hog\", aug_color_des_train, aug_shape_des_train, color_des_valid, shape_des_valid),\n",
    "    (\"color + texture\", aug_color_des_train, aug_texture_des_train, color_des_valid, textura_des_valid),\n",
    "    (\"hog + texture\", aug_shape_des_train, aug_texture_des_train, shape_des_valid, textura_des_valid),\n",
    "    (\"color + hog + texture\",\n",
    "     [np.concatenate([c, h, t]) for c, h, t in zip(aug_color_des_train, aug_shape_des_train, aug_texture_des_train)],\n",
    "     None,\n",
    "     [np.concatenate([c, h, t]) for c, h, t in zip(color_des_valid, shape_des_valid, texture_des_valid)],\n",
    "     None)\n",
    "]\n",
    "\n",
    "print(\"Evaluación de combinaciones multimodales con SVM:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, train1, train2, val1, val2 in combinacion_descriptores:\n",
    "    if train2 is None:\n",
    "        train_features = train1\n",
    "        val_features = val1\n",
    "    else:\n",
    "        train_features = [np.concatenate([a, b]) for a, b in zip(train1, train2)]\n",
    "        val_features = [np.concatenate([a, b]) for a, b in zip(val1, val2)]\n",
    "\n",
    "    model = SVC(kernel='rbf', C=10.0, probability=True)\n",
    "    model.fit(train_features, aug_train_labels)  # note: usando labels aumentado\n",
    "\n",
    "    predictions = model.predict(val_features)\n",
    "    f_score = f1_score(valid_labels, predictions, average='macro')\n",
    "\n",
    "    print(f\"F-score para la combinación {name}: {f_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c3316",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1b268776f0c532720e692f909c1a0e5",
     "grade": false,
     "grade_id": "cell-70db96924fefbb58",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# === Combinaciones con dataset AUMENTADO para entrenamiento ===\n",
    "combinacion_descriptores = [\n",
    "    (\"color + hog\", aug_color_des_train, aug_shape_des_train, color_des_valid, shape_des_valid),\n",
    "    (\"color + texture\", aug_color_des_train, aug_texture_des_train, color_des_valid, textura_des_valid),\n",
    "    (\"hog + texture\", aug_shape_des_train, aug_texture_des_train, shape_des_valid, textura_des_valid),\n",
    "    (\"color + hog + texture\",\n",
    "     [np.concatenate([c, h, t]) for c, h, t in zip(aug_color_des_train, aug_shape_des_train, aug_texture_des_train)],\n",
    "     None,\n",
    "     [np.concatenate([c, h, t]) for c, h, t in zip(color_des_valid, shape_des_valid, textura_des_valid)],\n",
    "     None)]\n",
    "\n",
    "print(\"Evaluación de combinaciones multimodales con SVM:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, train1, train2, val1, val2 in combinacion_descriptores:\n",
    "    if train2 is None:\n",
    "        train_features = train1\n",
    "        val_features = val1\n",
    "    else:\n",
    "        train_features = [np.concatenate([a, b]) for a, b in zip(train1, train2)]\n",
    "        val_features = [np.concatenate([a, b]) for a, b in zip(val1, val2)]\n",
    "\n",
    "    model = SVC(kernel='rbf', C=10.0, probability=True)\n",
    "    model.fit(train_features, aug_train_labels)  # note: usando labels aumentado\n",
    "\n",
    "    predictions = model.predict(val_features)\n",
    "    f_score = f1_score(valid_labels, predictions, average='macro')\n",
    "\n",
    "    print(f\"F-score para la combinación {name}: {f_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5f3338",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e44af69a4200ed65c3627ed455f2ed7b",
     "grade": false,
     "grade_id": "cell-5bdaf3f472f7aa38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Crea también una función Demo que reciba como parámetro una imagen, extraiga el o los descriptores correspondientes de la imagen y la clasifique utilizando el clasificador final entrenado. El código debe generar automáticamente las predicciones para el conjunto de Test utilizando la función Demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d09058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar los descriptores de entrenamiento (color + hog + texture)\n",
    "train_multimodal_desc = [\n",
    "    np.concatenate([c, h, t])\n",
    "    for c, h, t in zip(aug_color_des_train, aug_shape_des_train, aug_texture_des_train)\n",
    "]\n",
    "\n",
    "# Entrenar el mejor modelo\n",
    "final_model = SVC(kernel='rbf', C=10.0, probability=True)\n",
    "final_model.fit(train_multimodal_desc, aug_train_labels)\n",
    "\n",
    "assert final_model > baseline 'Necesita refinar su modelo multimodal'\n",
    "\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a0adb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc6d6bf8bc33134a7c124f5265253df1",
     "grade": false,
     "grade_id": "cell-b15be84f1ef66c23",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def Demo(Im: np.ndarray) -> int:\n",
    "    \"\"\"Función que predice la clase de una imagen\n",
    "\n",
    "    Args:\n",
    "        Im (numpy.ndarray): Imagen a predecir\n",
    "\n",
    "    Returns:\n",
    "        int: Clase predicha\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # === Preprocesamiento ===\n",
    "    resize_shape = (256, 256)\n",
    "    patch_sizes = [(256, 256), (128, 128), (64, 64)]\n",
    "    bins_color = 50\n",
    "    bins_texture = 25\n",
    "    hog_params = dict(\n",
    "        orientations=10,\n",
    "        pixels_per_cell=(8, 8),\n",
    "        cells_per_block=(2, 2),\n",
    "        visualize=False,\n",
    "        channel_axis=-1\n",
    "    )\n",
    "\n",
    "    # Convertir a RGB si es necesario\n",
    "    img_rgb = processing(Im, resize_shape, \"rgb\")\n",
    "\n",
    "    # === Extraer descriptores ===\n",
    "    color_desc = Color_pyramid(img_rgb, patch_sizes, \"concat\", bins_color)\n",
    "    hog_desc = hog(resize(img_rgb, (128, 64)), **hog_params)\n",
    "    texture_desc = texture_histogram(img_rgb, filterbank, model1, bins=bins_texture)\n",
    "\n",
    "    # === Concatenar descriptor multimodal ===\n",
    "    combined_desc = np.concatenate([color_desc, hog_desc, texture_desc]).reshape(1, -1)\n",
    "\n",
    "    # === Predecir con el mejor modelo ===\n",
    "    predict = final_model.predict(combined_desc)[0]\n",
    "\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62828f67",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fa9cc2438821146a3db6841c8fa8bd9",
     "grade": false,
     "grade_id": "cell-da3c4622f0447bb5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora, realizaremos predicción sobre las imágenes de test y calcularemos la f-medida para ver si cumple con las expectativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24575718",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict=[]\n",
    "test_labels_list=[]\n",
    "pbar = tqdm(total = len(test_paths))\n",
    "for path, label in zip(test_paths, test_labels):\n",
    "    Im=cv2.imread(path)\n",
    "    test_labels_list.append(label)\n",
    "    test_predict.append(Demo(Im))\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "f_test=f1_score(test_labels_list,test_predict,average='macro')\n",
    "precision = precision_score(test_labels_list, test_predict, average='macro')\n",
    "recall = recall_score(test_labels_list, test_predict, average='macro')\n",
    "\n",
    "# Imprimir resultados\n",
    "print(f\" F1-score  (macro): {f1:.4f}\")\n",
    "print(f\" Precision (macro): {precision:.4f}\")\n",
    "print(f\" Recall    (macro): {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a77a18e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "602ce75714fb4a3573dca47499c367ea",
     "grade": true,
     "grade_id": "cell-032d455fa63b0c12",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert (np.unique(test_predict) == np.arange(0, 5, 1)).all(), 'Las clases predichas no son las correctas'\n",
    "assert not np.isclose(np.std(test_predict), 0.0), 'Las predicciones no son las esperadas'\n",
    "print(f'Su metrica final en Test es: {f_test:.3f}')\n",
    "assert f_test>0.6, 'Su modelo no generaliza bien'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f888fd61",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da88b91b30d9b02c0fa132bc622be182",
     "grade": false,
     "grade_id": "cell-4d07c0545a624605",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Entregables\n",
    "Los entregables para esta entrega son los siguientes:\n",
    "\n",
    "- Jupyter notebook (.ipynb): El notebook debe estar completamente resuelto. Recuerden que la evaluación depende del correcto funcionamiento de su código.\n",
    "\n",
    "- Secciones requeridas del artículo (.pdf): La extensión máxima del artículo es de 10 páginas (4 páginas nuevas sumadas a las 6 de entregas anteriores). El formato del artículo está disponible en este [enlace](https://github.com/cvpr-org/author-kit). Si tienen dudas sobre cómo usar el formato, pueden consultar al Asistente Graduado.\n",
    "\n",
    "- Archivo de texto (.txt): Este archivo contendrá las secciones de código de su Jupyter notebook. Para más información sobre cómo convertir las secciones de código en un archivo de texto, revisen el video en el siguiente [enlace](https://uniandes-edu-co.zoom.us/rec/share/QOxUUIw7Uz9DsnKPyftOXZgM4bx5d7KHMYK-dqvGnkyJ2GfqvT7NU3lhQc0NtNSU.jveWB2H34S4uLIoB).\n",
    "\n",
    "## Artículo\n",
    "A medida que avanzamos en el desarrollo del proyecto, es fundamental continuar con la construcción progresiva del artículo. En esta entrega, se debe agregar lo que se realizó.\n",
    "\n",
    "### Resultados: Multi-descriptor\n",
    "Basado en su baseline, modifiquen el descriptor final para incluir información adicional de la imagen. Por ejemplo, si el baseline es un descriptor de forma, experimenten agregando información de color y/o textura, manteniendo el clasificador SVM. Sus experimentos deben incluir combinaciones de los tres tipos de información (color, textura y forma), aunque el modelo final no necesariamente debe usar los tres.\n",
    "\n",
    "Presenten los resultados de su experimentación (precisión, recall y F1-score), acompañados de una breve descripción de los resultados. Pueden guiarse respondiendo las siguientes preguntas:\n",
    "\n",
    ">- ¿Cuál es su novedad propuesta?\n",
    ">- ¿Alguna combinación de descriptores funcionó mejor que otras?\n",
    ">- ¿Existe alguna relación entre la longitud del descriptor y los resultados finales?\n",
    ">- ¿Qué clase fue mejor clasificada con su modelo final?\n",
    ">- ¿Algunos experimentos no mejoraron el baseline?\n",
    "\n",
    "### Resultados: Clasificadores\n",
    "Experimente modificando el clasificador y sus hiperparámetros. Agregue los resultados obtenidos y una breve descripción de los mismos. Para guiarse, respondan las siguientes preguntas:\n",
    "\n",
    ">- ¿Cuál fue su mejor método?\n",
    ">- ¿Hubo algún parámetro que influyera más en los resultados?\n",
    ">- ¿Qué combinación no funcionó correctamente?\n",
    ">- ¿Existía alguna tendencia?\n",
    ">- ¿Algún clasificador tuvo un desempeño mejor o peor que el baseline?\n",
    ">- ¿Cuál clasificador funcionó mejor?\n",
    "\n",
    "### Resultados: Método final\n",
    "Describan su modelo final junto con un Overview Figure que explique visualmente cómo funciona el modelo (input, procesamiento y output). La imagen debe mostrar cómo se extraen los descriptores finales, los parámetros importantes del modelo y el clasificador.\n",
    "\n",
    "Evaluén su modelo en el dataset de Test y agreguen las métricas de evaluación (precisión, recall y F1-score), tanto en total como por clase. Incluir imágenes donde la clasificación fue errónea.\n",
    "\n",
    "### Discusión: Multi-descriptor\n",
    "Incluyan un análisis de lo obtenido en sus experimentos (resultados cuantitativos y cualitativos). Teoricen sobre las mejoras o desmejoras de los modelos respondiendo las siguientes preguntas:\n",
    "\n",
    ">- ¿Cuáles son los sustentos teóricos de los resultados?\n",
    ">- ¿Por qué creen que algunas imágenes fueron mal clasificadas?\n",
    ">- ¿Por qué la clase con mejor desempeño tuvo esos resultados?\n",
    ">- ¿Por qué la clase con peor desempeño tuvo esos resultados?\n",
    "\n",
    "### Discusión: Clasificadores\n",
    "Analicen los resultados obtenidos con los clasificadores y respondan las siguientes preguntas:\n",
    "\n",
    ">- ¿Algún clasificador no mejoró el baseline? ¿Por qué?\n",
    ">- ¿Los resultados de los descriptores fueron consistentes en todos los clasificadores? ¿Por qué?\n",
    "\n",
    "### Discusión: Método final\n",
    "Realicen un análisis en profundidad de la combinación de parámetros y del descriptor final utilizado. Describan las características de los resultados obtenidos, si su modelo tiene alta precisión o cobertura, y las posibles implicaciones de esto en la vida real. Mencionen las limitaciones y posibles mejoras.\n",
    "\n",
    "### Conclusiones\n",
    "Concluyan sobre los resultados obtenidos y lo novedoso de su aproximación. ¿Creen que su modelo es suficiente? ¿Cómo lo mejorarían?\n",
    "\n",
    "## Atención\n",
    "En esta última entrega realizaremos una dinámica para incentivarlos a mejorar sus modelos y ser creativos. Habrá 2 bonos para los grupos cuya idea sea más creativa, aunque no tenga los mejores resultados. También habrá otros 2 bonos para aquellos equipos con la experimentación más exhaustiva, es decir, que puedan acercarse a obtener el mejor resultado posible para su conjunto de datos.\n",
    "\n",
    "Es importante que todos los grupos presenten sus resultados de Test. De lo contrario, habrá una penalización en el informe, que representa la mitad de la nota del Miniproyecto 3. Cualquier tipo de trampa, como entrenar en el dataset de Test, será penalizada con un 0 en el Miniproyecto 3.\n",
    "\n",
    "Mucha suerte a todos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f697c44d-b931-4b4f-8e91-01a323963867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "converter(\"Main2_ Acosta_Salazar_Vidales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd839316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
